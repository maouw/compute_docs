[
  {
    "objectID": "docs/storage/index.html",
    "href": "docs/storage/index.html",
    "title": "Storage",
    "section": "",
    "text": "This section describes different options for storing research data for the University of Washington Department of Psychology.\nFor information about storing data on Klone, see here.",
    "crumbs": [
      "Storage"
    ]
  },
  {
    "objectID": "docs/storage/index.html#comparison",
    "href": "docs/storage/index.html#comparison",
    "title": "Storage",
    "section": "Comparison",
    "text": "Comparison\n\nStorage options\n\n\n\nU Drive\nUW LOLO\nUW SFS\nUW OneDrive\nUW Google Drive\nRedCap\nResearchWorks\nAmazon S3\nAzure Blobs\nDropbox\nOSF\nGitHub\n\n\n\n\nSuitability\nInteractive use at UW\nLong-term storage, archival\nInteractive use and collaboration at UW\nInteractive use and collaboration at UW\nInteractive use and collaboration at UW\nResearch data\nResearch data publication at UW\nProgramming, sharing, archival\nProgramming, sharing, archival\nInteractive use and collaboration\nResearch data, collaboration, publishing\nCode, software containers, collaboration\n\n\nUW support\n\n\n\n\n\n\n\nüí≤discount 5%\nüí≤discount ~10%\n\n\n\n\n\nStorage cost\nFree up to 50 GiB, then $123/TiB/month ($0.12/GiB)\n$3.45/TiB/month\n$3.45/TiB/month\nFree, 5 TiB limit\nFree, 100 GiB limit\n\n\n~$25/month for 1 TiB hot data, ~$5/month for cold (S3 Glacier Instant Retrival)\n~$25/month for 1 TiB hot data, ~$5/month for cold; ~$30/TiB for cold egress\nSubscription ($12/month for 2 TiB)\nFree\nFree for most uses\n\n\nOff-campus availability\nVPN\nVPN\nVPN\n\n\n\n\n\n\n\n\n\n\n\nAutomatic backups\n\n\n\nLimited\nLimited\n\n\n\n\n\n\n\n\n\nVersioning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlatforms\n  \n  \n  \n   \n   \n\n\n   \n   \n    \n\n    \n\n\nEncryption in transit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncryption at rest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFERPA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHIPAA\n\n\n\n\n\n\n\nOn request\n\n\n\n\n\n\nLocal filesystem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSharing\n\nUW only\nUW only\nUW only\n\nUW only\n\n\n\n\n\n\n\n\nAvailable on Klone\n\n\n\nw/rclone\nw/rclone\n\n\nw/rclone\nw/rclone\nw/rclone",
    "crumbs": [
      "Storage"
    ]
  },
  {
    "objectID": "docs/storage/index.html#concepts",
    "href": "docs/storage/index.html#concepts",
    "title": "Storage",
    "section": "Concepts",
    "text": "Concepts\n\nStorage types\nThere are many different types of storage. Some of the most common types of storage are:\n\nObject storage\nObject storage is a type of storage that stores data as objects. Each object contains data, metadata, and a unique identifier. Objects are stored in a flat address space, and can be accessed using a URL. Object storage is designed to store large amounts of unstructured data. Object storage is not designed to be used as a file system. Object storage is typically accessed using a REST API. Object storage is typically used for storing data that is not actively being worked on, but needs to be stored for a long time. [Amazon S3][#amazon-web-services] and Azure Blobs are examples of object storage services.\n\n\nBlock storage\nBlock storage is a type of storage that stores data as blocks. Each block contains data and a unique identifier. Blocks are stored in a hierarchical address space, and can be accessed using a file system. Block storage is designed to store large amounts of structured data. Block storage is typically used for storing data that is actively being worked on. Amazon EBS is an example of block storage.\n\n\nFile storage\nFile storage is a type of storage that stores data as files. Your computer‚Äôs local filesystem uses file storage. Network filesystems such as NFS and SMB, and cloud storage services such as Azure Files and Amazon EFS also use file storage. File storage is designed to store large amounts of structured data. File storage is designed to be used by multiple users at the same time and supports creating, reading, updating, and deleting files. File storage is typically used for storing data that is actively being worked on. Azure Files is an example of file storage.\n\nSync services\nCloud-based file sync services, such as Dropbox, Google Drive, and OneDrive, offer file storage that can easily be accessed from multiple devices. These services usually offer convenient features such as file versioning, recovery, and file sharing. You can access the files stored in these services from a directory on your computer, and the files are automatically synchronized with the cloud storage service. However, the services may not support all of the features of a traditional file system (e.g.¬†file permissions, symbolic links) and often have restrictions on the length of file names and the types of characters that can be used in file names, which may cause serious problems when using files stored in these services with some applications (e.g., FreeSurfer). Caution is advised when working with files stored in these services.\n\n\n\nVersion control systems\nVersion control systems, such as Git, are designed to store and manage different versions of files. Version control systems are typically used for storing source code, but can also be used for storing other types of files. There are some version control systems that are designed to handle datasets, such as DVC. GitHub provides online hosting and collaboration services for Git repositories.\n\n\nData repositories\nData repositories, such as ResearchWorks Archive and OSF, are designed to store and manage research data and their metadata. Data repositories are typically used for storing data that is being shared with other researchers. Data repositories often provide features such as versioning, recovery, sharing, and embargos. Data repositories often provide a DOI for each dataset, which can be used to cite the dataset in publications.\n\n\n\nCost considerations\nPricing for storing and accessing data are usually influenced by the following factos:\n\nStorage capacity\nHow much data is being stored. The more data that is being stored, the more it will cost to store the data. Storage services may have minimum and/or maximum storage capacity requirements, and may charge for storage in increments of a certain size (e.g.¬†1 TiB).\n\n\nStorage latency\nHow quickly data can start being read or written. The lower the latency, the faster data can be read or written. Storage services may charge more for lower latency. A solid-state drive (SSD) has lower latency than a hard disk drive (HDD), which has lower latency than a tape drive. However, capacity on SSDs is more expensive than capacity on HDDs, which is more expensive than capacity on tape drives.\n\n\nData transfer\nThe rate at which data can be downloaded from or uploaded to a service may be limited. The higher the bandwidth, the faster data can be downloaded from or uploaded to the service. Service providers may charge more for higher bandwidth.\nData transfer costs may be charged for uploading data to and downloading data from a service, also known as ingress and egress. Typically, ingress is free and egress is charged. Some services may charge more for uploading data to and downloading data from certain locations (e.g.¬†outside of the United States). Some providers offer options such as physical media transfer (e.g.¬†mailing a hard drive) for uploading and downloading large amounts of data (see AWS Snowball).\n\n\nRetention period\nHow long data needs to be stored. The longer data needs to be stored, the more it will cost to store the data. Billing for storage services is typically done on a monthly or annual basis.\n\n\nBackup and recovery options\nHow often data is backed up, how long backups are kept, and how quickly data can be recovered from backups. The more often data is backed up, the longer backups are kept, and the faster data can be recovered from backups, the more it will cost to store the data. Some services may not provide backup and recovery options, or may charge extra for backup and recovery options.\n\n\nFrequency of access\nHow often data is accessed. Some services may charge for each time data is accessed, or may charge more for more frequent access.\n\n\nData access restrictions\nWho can access the data. Data access restrictions may be inherent to a service (e.g., only people connected to the UW intranet can access the data) or may be imposed by the user, the organization, or the service provider. Sharing data with outside collaborators may require additional steps such as creating accounts for the collaborators, or may not be possible at all.\n\n\nData security\nHow secure the data is against unauthorized access. While access restrictions can limit who can access the data in certain circumstances, measures such as encryption may be necessary to prevent access by users who have access to the data but should not be able to access the data (e.g., system administrators).\nResearch data that is subject to HIPAA or FERPA regulations must be stored in a HIPAA or FERPA compliant service. For more information about HIPAA at UW, see the HIPPA Guidance page.",
    "crumbs": [
      "Storage"
    ]
  },
  {
    "objectID": "docs/storage/index.html#options",
    "href": "docs/storage/index.html#options",
    "title": "Storage",
    "section": "Options",
    "text": "Options\n\nUW-supported options\nUW provides a number of different storage options for faculty, staff, and students. Additionally, there are a number of storage options that are not directly supported by UW, but are available to UW users through UW‚Äôs enterprise agreements with the service providers, sometimes at a discount.\nThe following sections describe some of the most common options. UW IT maintains a page that provides an overview of UW‚Äôs online storage options, as well as a comparison of file service options. The UW IT Service Catalog has a more complete list of UW IT services.\nUW Libraries‚Äô Research Data Services also maintains a page with information about storage options.\n\nDepartmental storage\nThe Department of Psychology has a server that can be used for storing research data. This server is located in Guthrie Hall and is managed by the department‚Äôs IT staff. The server is not intended for sharing data with collaborators outside of the University of Washington. Contact the department‚Äôs IT staff for more information.\n\nFeatures and options\n\n2 TiB of storage (up to 10 TiB available on request) per lab\nAccessible off campus via VPN\nWeekly backups (twice a week on request) to a backup server\nMonthly backups to network-attached storage (NAS)\nStorage is not encrypted\nShared access typically by sharing a single username and password per lab\n\nAlternative configurations are possible on requuest (e.g., read-only accounts, restricted access to specific directories)\n\nMountable as a drive on Windows, Mac, and Linux systems\n\n\n\nPricing\nFree for departmental labs up to 10 TiB. More available on request.\n\n\nEligibility\n\nDepartment faculty and staff\n\n\n\n\nUW LOLO\nThe UW LOLO Data Archive provides long-term tape-backed archival storage for users at the university. It is only intended for use by UW faculty, staff, and affiliated organizations. It is suitable for data that s not actively being worked on. The Hyak documentation has additional information about the LOLO Data Archive.\n\nFeatures and options\n\n\nAll files immediately and directly accessible via SSH protocol\nSupport for up to 1,000 files per TB of data stored\nFast uploads and downloads for large (&gt;=100GB) files\n10Gbs network connection to campus, the internet, and Hyak\nTwo copies of all files are preserved, each in a separate data center, each in a different seismic zone\n\n\n\n\nPricing\nThe LOLO Data Archive is priced at $3.45/TiB/month, with a minimum purchase of 1 TiB. The minimum purchase is $3.45/month. The LOLO Data Archive is billed monthly. The LOLO Data Archive is billed to a UW Workday worktag.\n\n\nEligibility\n\nUW faculty, staff, and affiliated organizations\nUW Workday worktag required\n\n\n\n\nUW Shared File Service\nThe UW Shared File Service provides a network file system that can be mounted on Linux and Windows systems. It is only intended for use by UW faculty, staff, and students. It is suitable for data that is actively being worked on.\n\nFeatures and options\n\n\nCIFS/SMB access from UW campus subnets. (Can also be accessed via VPN Service, which provides remote systems with a UW campus IP address.)\nSFTP access from any internet location.\nUser self-service file restores via ‚Äúsnapshots‚Äù\nDisaster recovery backups in two geographically separate regions (Seattle and Spokane). Per-file restores from tape are not available.\nEasy allocations of additional space as demand requires.\nAccess controlled via UW NetIDs and UW Groups\nPermissions are limited to Read/Write for a single group per folder, full granular ACLs are not supported currently.\n\n\n\n\nPricing\n$0.25/GiB/month\n\n\nEligibility\n\nUW faculty; UW staff; UW researchers; UW clinicians; UW academic units; UW administrative units; UW Medical Center; Harborview Medical Center; requires a valid UW budget number\n\n\n\nCompliance\n\nThis service is considered FERPA compliant by the UW Registrar‚Äôs office (due to the process used for data release), but has had no other formal third party data security or privacy compliance audits.\n\n\n\n\nUW U Drive\nThe UW U Drive provides a network file system that can be mounted as a drive on Windows, Mac, and Linux 0systems on campus or by VPN.\n\nFeatures and options\n\n\nCIFS/SMB access from UW campus subnets. (Can also be accessed via VPN Service, which provides remote systems with a UW campus IP address.)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSFTP access to U Drive will no longer be available after March 20, 2024.\n\n\n\n\nPricing\nFree up to 50 GiB. $0.12/GiB/month for additional storage.\n\n\nEligibility\n\nUW faculty; UW staff; UW students; UW students in residence halls; UW researchers\n\n\n\n\nUW OneDrive for Business\nUW provides access to the Microsoft OneDrive for Business cloud file syncing service as part of its Office 365 subscription.\n\nFeatures and options\n\n5 TiB of storage\n250 GiB maximum file size\nHIPAA and FERPA compliance\nSharing with UW users\nDesktop, browser, and mobile access\nSharePoint Online integration\nAccessible on Klone via rclone\n\n\n\n\n\n\n\nWarning\n\n\n\nOneDrive has technical limitations that may cause problems when using it with some applications (e.g., FreeSurfer). Caution is advised when working with files stored in OneDrive. The support page has more information about OneDrive‚Äôs limitations.\n\n\n\n\nPricing\nFree for eligible UW users.\n\n\nEligibility\n\nUW faculty; UW staff; UW students; UW researchers; UW clinicians; For Shared UW NetIDs and Sponsored & affiliate UW NetIDs, accounts can be provisioned by UW employees\n\n\n\n\nUW Google Drive\nUW Google Drive provides Google Drive file storage and sync for users at UW (this is not the same as the Google Drive service provided by Google).\n\nFeatures and options\n\n100 GiB of storage\nFERPA compliance\nAccessible on Klone via rclone\n\n\n\n\n\n\n\nWarning\n\n\n\nUW Google Drive is not HIPAA compliant.\n\n\n\n\n\nResearchWorks Archive\n\nResearchWorks Archive is the University of Washington‚Äôs digital repository (also known as ‚Äúinstitutional repository‚Äù) for disseminating scholarly work. More information about ResearchWorks can be found on the Scholarly Publishing Services page.\n\nLink: ResearchWorks Archive\n\n\nRedCap\nRedCap is a web-based application for building and managing online surveys and databases. According to the RedCap website:\n\nResearch Electronic Data Capture (REDCap) is a rapidly evolving web tool developed by researchers for researchers in the translational domain.\nREDCap features a high degree of customizability for your forms and advanced user right control. It also features free, unlimited survey functionality, a sophisticated export module with support for all the popular statistical programs, and supports HIPAA compliance.\n\n\n\nCloud storage\n\nAmazon Web Services\nAmazon Web Services (AWS) is a cloud computing platform that provides a number of different storage services, including Amazon S3, Amazon EBS, Amazon EFS, and Amazon Glacier. Amazon S3 is comparable to Azure Blobs. Amazon EBS is comparable to Azure Files. Amazon EFS is comparable to Azure Files. Amazon Glacier is comparable to Azure Archive Storage.\n\nFeatures and options\n\n\nData Egress Waiver, which effectively eliminates the standard charges for moving data out of the AWS Service.\nHIPAA Eligible Account, requires special request and approval, and is subject to important operational considerations to meet compliance requirements of HIPAA, the UW BAA, and UW Medicine Compliance Policies.\nAccessible on Klone via rclone\n\n\n\n\nPricing\nThe approximate cost of storing 1 TiB of data per month is: - ~$25 for ‚Äúhot‚Äù data accessed frequently (S3 Standard) - ~$5/month for ‚Äúcold‚Äù data accessed infrequently (S3 Glacier Instant Retrieval)\nThe Data Egress Waiver eliminates the standard charges for moving data out of the AWS Service.\nUW IT has a subscription service for AWS that provides a discount of 5% and a waiver for data egress charges. The subscription is covered by UW‚Äôs HIPPAA BAA and other enterprise contracts. A UW Workday worktag is required to use the service. For more information, see the UW IT AWS page.\nFor detailed pricing information, see the AWS pricing calculator.\n\n\nEligibility\n\nUW faculty; UW staff; UW researchers; UW clinicians; UW academic units; UW administrative units; UW affiliated organizations; UW Medical Center; Harborview Medical Center; Any group with an approved UW blanket PO. NOTE: UW students are not eligible for this service, except when working within the scope of UW employment, e.g., as an RA, TA or GSA. Known Prerequisites: This service requires a valid UW budget number.\n\n\n\n\nAzure\nMicrosoft‚Äôs Azure cloud platform includes a number of storage options, including Azure Blobs and Azure Files.\nAzure Blobs is comparable to Amazon AWS S3. It is designed to store large amounts of unstructured data but is not designed to be used as a file system.\n\nFeatures and options\n\nCovered by UW‚Äôs HIPPAA BAA and other enterprise contracts\nAccessible on Klone via rclone\n\n\n\nPricing\nThe approximate cost of storing 1 TiB of data per month is: - ~$25 for ‚Äúhot‚Äù data accessed frequently (Azure Blobs Hot Tier) - ~$5/month for ‚Äúcold‚Äù data accessed infrequently (Azure Blobs Cold Tier)\nUW IT has a subscription service for Azure that provides a discount of roughly 10% on Azure usage charges and allows charges to be paid with a UW Workday worktag. A UW Workday worktag is required to use the service. For more information, see the UW IT Azure Subscription Service.\nFor detailed pricing information, see the Azure pricing calculator.\n\n\nEligibility\n\nUW faculty; UW staff; UW researchers; UW clinicians; UW academic units; UW administrative units; UW affiliated organizations; UW Medical Center; Harborview Medical Center; Any group with an approved UW blanket PO. NOTE: UW students are not eligible for this service, except when working within the scope of UW employment, e.g., as an RA, TA or GSA. Known Prerequisites: This service requires a valid UW budget number.\n\nUW-IT‚Äôs Azure Subscription service allows UW units, that wish to create and manage their own Azure subscription, to receive a discount (~10%) on Azure usage charges and to pay those with a UW Workday worktag. ThesUW‚Äôs HIPAA BAA and associated enterprise contracts to cover these subscriptions.\n\n\n\n\n\nOther options\n\nGoogle Cloud Storage\nGoogle Cloud Storage is a cloud object storage service that is part of the Google Cloud Platform. It resembles the offerings by AWS and Azure. It is distinct from UW Google Drive and Google Drive. Google Cloud Storage can be accessed on Klone via rclone.\n\n\n\n\n\n\nTip\n\n\n\nGoogle Cloud also offers block storage, file storage, and archival storage services. See here for more information.\n\n\n\n\nOpen Science Framework\nOpen Science Framework (OSF) is a web-based platform for managing research projects. OSF can be used to store research data and their metadata. It is best suited for collaborating with other researchers and sharing research data for papers. Storage addons are available to connect Amazon S3, Dropbox, GitHub, OneDrive, and GitHub to OSF.\n\n\nDropbox\nDropbox is a subscription-based cloud file sync service. The University of Washington does not support Dropbox. Dropbox is not suited for storing sensitive data. However, Dropbox can be useful for storing non-sensitive data that is actively being worked on, and provides convenient versioning and recovery options. Dropbox can be accessed on Klone via rclone.\n\n\nGitHub\nGitHub is a web-based hosting service for version control using Git. Files up to 100 MiB each can be stored in a GitHub repository. The total size of a repository should be under 5 GiB.\nGitHub repositories can be made publicly available, or can be made private. Private repositories are only accessible to collaborators who have been granted access to the repository. Private repositories are only available to collaborators who have GitHub accounts. GitHub repositories can be accessed using the GitHub website, or using the GitHub Desktop application.\n\nGitHub Releases\nGitHub Releases can be used to store files up to 2 GiB in size each.\n\n\nGitHub Packages\nGitHub Packages can be used to create container that can be uploaded to the GitHub Container Registry. For public repositories, there are no charges nor storage limits. For private repositories, storage limits and charges apply. A GitHub account is required to access GitHub Packages.\nAlthough GitHub Packages is not designed to store research data, it can be used to store research data in the form of a container containing an archive of research data (similar to a ZIP file). The data cannot be viewed or modified while stored in GitHub Packages, but can be downloaded and extracted from the container. Any changes to the data require creating a new image and uploading it to GitHub Packages.\nPublic packages can be accessed by anyone and should not be used for sensitive data. However, it may be possible to encrypt data before storing it in the image, or to deploy an encrypted container.\n\n\nLarge File Storage\nGitHub has a Large File Storage (LFS) extension that can be used to store large files in a GitHub repository.\nAccording to GitHub‚Äôs documentation, the file size limits for GitHub repositories are:\n\n\n\nPlan\nMaximum file size\n\n\n\n\nGitHub Free\n2 GiB\n\n\nGitHub Pro\n2 GiB\n\n\nGitHub Team\n4 GiB\n\n\nGitHub Enterprise Cloud\n5 GiB",
    "crumbs": [
      "Storage"
    ]
  },
  {
    "objectID": "docs/start/introduction.html",
    "href": "docs/start/introduction.html",
    "title": "Introduction to UW Hyak",
    "section": "",
    "text": "HYAK is the University of Washington‚Äôs high-performance computing cluster: &gt; HYAK is part of the University of Washington‚Äôs cyberinfrastructure plan to support world-class research in every department. HYAK is an ecosystem of high-performance compute (HPC) clusters and supporting infrastructure (e.g., data management, computational training, scientific consulting). Acknowleding our Pacific Northwest heritage, HYAK means ‚Äúfast‚Äù in Chinook jargon. Counting 1, 2, and 3 translates to ‚Äúikt‚Äù, ‚Äúmox‚Äù, and ‚Äúklone‚Äù in this local trading language. The numbers are cluster names corresponding to the generation of HYAK cluster deployed. We are currently on the 3rd generation cluster for HYAK.\n\n\nThe latest generation of the HYAK cluster is Klone.\n\n\n\nThe second generation of the HYAK cluster, Mox, is being phased out in 2024. No new capacity or node additions are planned. The Klone cluster is recommended for all users.\n\n\n\nCheck out UW Research Computing Club‚Äôs Slack and ask questions under hyak-questions.\nFor more information, see the official HYAK website.",
    "crumbs": [
      "Getting Started",
      "Introduction to UW Hyak"
    ]
  },
  {
    "objectID": "docs/start/introduction.html#klone",
    "href": "docs/start/introduction.html#klone",
    "title": "Introduction to UW Hyak",
    "section": "",
    "text": "The latest generation of the HYAK cluster is Klone.",
    "crumbs": [
      "Getting Started",
      "Introduction to UW Hyak"
    ]
  },
  {
    "objectID": "docs/start/introduction.html#mox",
    "href": "docs/start/introduction.html#mox",
    "title": "Introduction to UW Hyak",
    "section": "",
    "text": "The second generation of the HYAK cluster, Mox, is being phased out in 2024. No new capacity or node additions are planned. The Klone cluster is recommended for all users.",
    "crumbs": [
      "Getting Started",
      "Introduction to UW Hyak"
    ]
  },
  {
    "objectID": "docs/start/introduction.html#resources",
    "href": "docs/start/introduction.html#resources",
    "title": "Introduction to UW Hyak",
    "section": "",
    "text": "Check out UW Research Computing Club‚Äôs Slack and ask questions under hyak-questions.\nFor more information, see the official HYAK website.",
    "crumbs": [
      "Getting Started",
      "Introduction to UW Hyak"
    ]
  },
  {
    "objectID": "docs/start/index.html",
    "href": "docs/start/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "Get access to Hyak:\n\nActive UW students can join UW Reserach Computing Club (RCC) to gain access to STF-funded nodes.\nPI‚Äôs or labs should purchase dedicated storage and compute resources.\n\nLogin to a cluster via SSH:\n\n\nKlone\n\n\nssh UWNetID@klone.hyak.uw.edu\n\n\n\n\nNavigate and understand its compute infrastructure:\n\nRequest compute jobs with Slurm\nAccess certain programs with Lmod software modules\nBuild and run software containers with Apptainer\nCreate and access a virtual desktop with hyakvnc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to UW Hyak\n\n\n\n\n\nAn introduction to using the University of Washington Hyak Cluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConnecting to HYAK with SSH\n\n\n\n\n\nHow to connect to HYAK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhyakvnc\n\n\n\n\n\nA program to create and manage VNC sessions running within a containerized Apptainer environment on a compute node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorage on Klone\n\n\n\n\n\nOptions and best practices for managing storage on Klone\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "docs/compute/apptainer.html",
    "href": "docs/compute/apptainer.html",
    "title": "Apptainer",
    "section": "",
    "text": "Apptainer (formerly Singularity) is a simple container platform enabling users to install and run software that would otherwise be unsupported by the host environment.\nOn Klone, the apptainer command is available on all compute nodes, but it is not available on the login node.\nBy default, the apptainer command will use the system version of Apptainer:\nwhich apptainer # prints /usr/bin/apptainer\napptainer --version # apptainer version 1.2.4-1.el8 (as of 2023-12-05)\n\n\n\n\n\n\nLoading a different version of Apptainer using Lmod\n\n\n\n\n\nTo use a different version of Apptainer, load the appropriate Lmod module. For example, to use Apptainer version 1.1.5:\nmodule load apptainer/1.1.5\nwhich apptainer # prints /sw/apptainer/1.1.5/bin/apptainer\napptainer --version # apptainer version 1.1.5\nTo see all available versions of Apptainer, run:\nmodule -t spider apptainer\nThe default module is indicated by the tag (D) following the version. For example, apptainer/local (D) indicates that the default version is local. (The local version is the version installed on the system.)\n\n\n\n\n\n\n\n\n\nApptainer containers are designed to be portable, so by default they do not have access to any files on the host system. To make files available to the container, you must bind them to the container. This is done using the --bind option of the apptainer command or the APPTAINER_BINDPATH environment variable.\n\n\n\n\n\n\nDefault bind paths\n\n\n\nBy default, apptainer will make several directories available within the container by binding them to the same path in the container, so that /somefolder on the host is available as /somefolder within the container.\nThese directories include: - $HOME (your home directory, a.k.a. ~) - /tmp (temporary directory ‚Äì unique to each node, contents are purged when the users‚Äô last job running on the node completes) - $PWD (the current working directory, i.e.¬†the directory you are in when you run apptainer)\nThe Klone Apptainer installation is also configured to bind several other following directories to the same path in the container, including:\n\n/mmfs1 (the main filesystem)\n/scr (the scratch filesystem, same as /tmp)\n\n\n\nWe recommend setting the following binds before running a container:\nexport APPTAINER_BINDPATH=\"/gscratch\" # Make /gscratch available within the container\n\nTo start an Apptainer container interactively, run the following:\n\napptainer shell &lt;path_to_container&gt;\n\n\n\n\n\napptainer pull docker://&lt;image_name&gt;[:&lt;tag&gt;] # Pulls the image from docker registry\n\n\n\n\n\nLet‚Äôs try running a Python script using Apptainer. The release notes for Python version 3.11 say that it is ‚Äúbetween 10-60% faster than Python 3.10‚Äù. Is this true? Let‚Äôs find out! We‚Äôll write a simple Python script to test this and use Apptainer to run it on Python 3.10 and Python 3.11.\n\n\nspeedtest.py\n\n#!/usr/bin/env python3\nimport os, sys, timeit\n\n# Get the values of the environment variables M and N, or use default values:\n1n = int(os.getenv(\"N\", default=1000))  # How many numbers to join\nm = int(os.getenv(\"M\", default=100))  # How many times to run the test\n\n# Function to test:\ndef join_nums():\n    return \"-\".join([str(i) for i in range(n)])\n\n2# Print details about the test to stderr:\nprint(\n    f\"Running join_nums() {n}*{m} times on Python v{sys.version_info.major}.{sys.version_info.minor}\",\n    file=sys.stderr,\n)\n\n# Run the test:\n3result = timeit.timeit(join_nums, number=m)\n\n# Print the result:\n4print(result)\n\n\n1\n\nThe os.getenv function retrieves the value of an environment variable or returns a default value if the variable is not set.\n\n2\n\nHere, we use the file parameter to print(), which makes it write to the standard error stream (stderr) instead of the default standard output stream (stdout). This is useful because we want to print the result of the test to stdout so that we can save the output by redirecting it to a file, but we also want to print some information about the test to stderr so that it doesn‚Äôt get mixed up with the result.\n\n3\n\ntimeit.timeit() runs a function multiple times and returns the average time it took to run the function.\n\n4\n\nThe print function prints to the standard output stream (stdout) by default. We can redirect this to a file in bash using the &gt; operator.\n\n\nNow let‚Äôs run the script using Python 3.10 and Python 3.11. For the image, we will use the python:3.10-slim and python:3.11-slim images from the official Python images.\n\n\n\n\n\n\nTagged container releases\n\n\n\nThe python:3.10-slim and python:3.11-slim images are tagged with the version of Python they contain. This means that if you pull the python:3.10-slim image today, it will always contain Python 3.10, even if Python 3.11 is released tomorrow.\nThe python:*-slim images are designed to contain only the minimal set of packages required to run Python and are therefore much smaller than the standard Python images (~45 MiB vs ~350 MiB).\n\n\nWe‚Äôll use the apptainer exec command to run the script inside an Apptainer container:\n1apptainer exec docker://python:3.10-slim python3 speedtest.py\n\n1\n\nThe apptainer exec command runs a command inside an Apptainer container. The docker://python:3.10-slim argument tells Apptainer to use the python:3.10-slim image from the Docker registry. The python3 ./speedtest.py argument tells Apptainer to run the python3 command inside the container and provide it with the argument speedtest.py.\n\n\nYou should see something like:\n1Running join_nums() 1000*100 times on Python v3.11\n20.003154174002702348\n\n1\n\nThis is printed to stderr because we used print(..., file=sys.stderr) in the script.\n\n2\n\nThis is printed to stdout.\n\n\nIt probably didn‚Äôt take very long to run the script. Let‚Äôs try running it again with larger values of M and N. We can set the values of M and N by setting them as environment variables in the bash command prompt before running the script:\n1export M=1000 N=100_000\napptainer exec docker://python:3.10-slim python3 speedtest.py\n\n1\n\nThe export command sets the values of the M and N environment variables and makes them avaialble to other programs like apptainer. We‚Äôre setting N to100_000 instead of 100000 because underscores can be used in Python to make large numbers easier to read. This is a feature of Python, not the shell (which interprets 100_000 as just a string and not a number).\n\n\nIt should take a bit longer to run this time.\nNow let‚Äôs try running the script using Python 3.11:\napptainer exec docker://python:3.11-slim python3 speedtest.py\nWe don‚Äôt need to set the values of M and N again because they are still set from the previous command via export (unless you closed your terminal window or logged out).\nIt probably took a bit less time to run the script this time.\nIs Python 3.11 really faster than Python 3.10? Let‚Äôs find out by redirecting the output of the script to a file:\n1apptainer exec docker://python:3.10-slim python3 speedtest.py &gt; py3.10.txt\napptainer exec docker://python:3.11-slim python3 speedtest.py &gt; py3.11.txt\n2cat py3.10.txt py3.11.txt # show the results\n\n1\n\nIn bash, the &gt; operator redirects the output of a command to a file. If the file already exists, it will be overwritten. If you want to append to an existing file instead, use the &gt;&gt; operator. These operators can be used with any command, not just apptainer. For example, ls &gt; my_files.txt will write the output of ls to the file my_files.txt.\n\n2\n\nThe cat command prints the contents of a file to stdoutIf you want to print the contents of multiple files, you can list them all as arguments tocat`.\n\n\nWe see the results, but they‚Äôre not very easy to interpret. Let‚Äôs pipe the output to Python to calculate the speedup:\n1cat py3.10.txt py3.11.txt | apptainer exec docker://python:3.11-slim python3 -c 'print(float(input())/float(input()))'\n\n1\n\nThe | operator pipes the output of one command to the input of another. In this case, we are piping the output of cat py3.10.txt py3.11.txt to the input of apptainer exec docker://python:3.11-slim python3 -c 'print(float(input())/float(input()))'. The -c option of the python3 command tells Python to run the code provided as an argument. The code print(float(input())/float(input())) reads two lines of input from standard input, converts them to floating point numbers, divides the first by the second, and prints the result.\n\n\nIn my case, Python 3.11 was about 1.3 times faster than Python 3.10. Not bad!\nHere‚Äôs a demo of the above commands. Note that I used M=200 and N=50_000 instead of M=1000 and N=100_000 because it takes a long time to run the script with the larger values of M and N.\n\n\n\n\n\n\nWhat if we wanted to add a command-line interface to make it possible to run the script with different values of M and N without having to set them as environment variables? A user might want to set the values of M and N as command-line arguments. Maybe they would also like to be able to specify the output file instead of redirecting the output to a file. And how about a progress bar? Users love progress bars!\nThis sounds like a tall order, but it‚Äôs actually quite easy to do in Python using the click package for Python. Here‚Äôs the new script:\n\n\nspeedtest-cli.py\n\n#!/usr/bin/env python3\nimport os, sys, timeit\nimport click\nfrom math import sqrt\n\n@click.command()  # Set up the context for the command line interface\n@click.option(\n    \"-n\",  # The name of the option\n    envvar=\"N\",  # Use the environment variable N if it exists\n    default=1000,  # Default value if N is not set\n    help=\"How many numbers to join\",  # Help text for the -n option\n    type=int,  # Convert the value to an integer\n)\n@click.option(\n    \"-m\",  # The name of the option\n    envvar=\"M\",  # Use the environment variable M if it exists\n    default=100,  # Default value if M is not set\n    help=\"How many times to run the test\",  # Help text for the -m option\n    type=int,  # Convert the value to an integer\n)\n@click.option(\n    \"--output\",\n    envvar=\"OUTPUT_FILE\",  # Use the environment variable OUTPUT_FILE if it exists\n    default=\"/dev/stdout\",  # Default value if OUTPUT_FILE is not set\n    help=\"Output file to write the results to\",\n    type=click.Path(writable=True, dir_okay=False),\n)\ndef speedtest(m, n, output):\n    \"\"\"Run a speed test.\"\"\"  # Help text for the command\n\n    bar = click.progressbar(length=n * m,\n        update_min_steps=sqrt(n * m), # How often to update the progress bar\n        label=\"Joining numbers\",\n        file=sys.stderr,\n    )\n\n    # Function to test:\n    def join_nums():\n        result = \"-\".join([str(i) for i in range(n)])\n        bar.update(n)\n        return result\n\n    # Print details about the test to stderr:\n    print(\n        f\"Running join_nums() {n}*{m} times on Python v{sys.version_info.major}.{sys.version_info.minor}\",\n        file=sys.stderr,\n    )\n    # Run the test:\n    result = timeit.timeit(join_nums, number=m)\n    bar.render_finish()\n\n    # Print the result:\n    with open(output, \"w\") as f:  # Open the output file for writing\n        print(result, file=f)\n    print(f\"Result written to {output}\", file=sys.stderr)\n\n# Run the command:\nspeedtest()\n\nNow, let‚Äôs try running the script with apptainer exec:\napptainer exec docker://python:3.11-slim python3 speedtest-cli.py\nYou probaby got a ModuleNotFoundError because the click package is not installed in the python:3.11-slim image. If you know some Python, you might think, ‚ÄúI could probably fix this if I install the click module via pip.‚Äù And you would be right ‚Äì if you were running the script in the version of Python that‚Äôs normally installed on your computer. But we‚Äôre running the script inside an Apptainer container, so we need to install the click module inside the container. Furthermore, we‚Äôre using two different versions of Python, so we need to install the click module in the containers for both versions.\nWe‚Äôll solve this by writing an Apptainer definition file to build a custom Apptainer image that contains the click module. Here‚Äôs the definition file:\n\n\nspeedtest.def\n\nBootstrap: docker # Specify where to get the container from.\nFrom: python:{{PY_VERSION}} # Which container to use as a base.\n\n# Define the arguments that can be passed at `apptainer build` time:\n%arguments\n    # This replaces {{PY_VERSION}} in the `From:` line above:\n    PY_VERSION=3.10\n  \n%files\n    # Copy the Python script to the container as the command `speedtest`:\n    speedtest-cli.py /opt/local/bin/speedtest\n    \n%environment\n    # Set the virtual environment as the default Python environment:\n    export PATH=\"/opt/venv/bin:$PATH\" # Add the virtual environment to the PATH.\n    export PATH=\"/opt/local/bin:$PATH\" # Add the directory with the `speedtest` command to the PATH.\n\n%post\n    # Create a virtual environment in /opt/venv to install our dependencies:\n    python -m venv /opt/venv\n\n    # Set the virtual environment as the default Python environment for the rest of the script:\n    # (the %environment section does not apply to the %post section)\n    export PATH=\"/opt/venv/bin:$PATH\"\n\n    # Install `click` and don't cache the downloaded files:\n    pip install --no-cache-dir click\n\n    # Print a message to stderr to let the user know that the installation is done:\n    echo \"$(python --version): Done installing dependencies.\" &gt;&2\n    # (python --version prints the result of the command `python --version`.)\n    # (The &gt;&2 part redirects the output to stderr instead of stdout.)\n\n    chmod +x /opt/local/bin/speedtest # Make the `speedtest` command executable.\n\n%runscript\n    # Run the Python script with the arguments passed to the container:\n    speedtest \"$@\"\n\nNow we can build the image:\napptainer build speedtest-py3.10.sif speedtest.def\nAnd now we can run the script using the new image with the apptainer run commanad, which is similar to apptainer exec but runs the command specified in the definition file instead of the python command specified as an argument:\napptainer run speedtest-py3.10.sif\nTry it with some different values of M and N:\napptainer run speedtest-py3.10.sif -m 30_000 -n 1000\nNow try specifying a different output file:\napptainer run speedtest-py3.10.sif -- -m 30_000 -n 1000 --output py3.10.txt\ncat py3.10.txt # Show the results\nHow do we build the image for Python 3.11? We could copy the definition file and change PY_VERSION to 3.11, but that would be a lot of work. Instead, we can use the --build-arg option of the apptainer build command to pass the value of PY_VERSION as a build argument to the definition file:\napptainer build --build-arg PY_VERSION=3.11 speedtest-py3.11.sif speedtest.def\nNow we can run the script using the new image:\napptainer run speedtest-py3.11.sif -m 30_000 -n 1000\nWe can still use the environment variables M and N to set the values of m and n:\nexport M=30_000 N=1000\napptainer run speedtest-py3.11.sif\nThis makes it easier to run both containers with the same values of M and N without having to specify them each time.\nIf we wanted to, we could even use OUTPUT_FILE to specify the output file instead of using the --output option, because we let click know that OUTPUT_FILE is an alternative for the --output argument if there is no --output option:\nexport OUTPUT_FILE=py3.11-2.txt\napptainer run speedtest-py3.11.sif\nunset OUTPUT_FILE # Unset the OUTPUT_FILE environment variable so that it doesn't affect the next command\nBecause we specified the %runscript section in the definition file, we can also execute the script directly without having to specify the apptainer command:\n./speedtest-py3.11.sif -m 30_000 -n 1000\nLet‚Äôs recreate the comparison we did earlier:\nexport M=1000 N=100_000\napptainer run speedtest-py3.10.sif --output py3.10.txt\napptainer run speedtest-py3.11.sif --output py3.11.txt\n1cat py3.10.txt py3.11.txt | apptainer exec speedtest-py3.10.sif python3 -c 'print(float(input())/float(input()))'\n\n1\n\nWe‚Äôre using apptainer exec instead of apptainer run because we want to run the python3 command instead of the command specified in the definition file.\n\n\n\n\n\n\nApptainer caches all the images it downloads in a cache directory to avoid downloading them again. The cache can get quite large, so it‚Äôs a good idea to clear it from time to time.\nYou can see the size of the cache directory by running:\napptainer cache list\nTo clear the cache, run:\napptainer cache clean\nThat should free up some space.\n\n\n\nBy default, Apptainer stores the cache in ~/.apptainer/cache. This can be a problem if you have a small home directory (e.g.¬†if you are using the default 10 GB quota on Klone). You can change the cache directory by setting the APPTAINER_CACHE environment variable. For example, to set the cache directory to /tmp/&lt;your-username&gt;/apptainer-cache, you can use the $USER environment variable:\nexport APPTAINER_CACHE=\"/tmp/$USER/apptainer-cache\"\nApptainer will create the cache directory if it does not already exist.\nYou‚Äôll need to set the APPTAINER_CACHE environment variable every time you want to use Apptainer, so it‚Äôs a good idea to add it to your ~/.bashrc file so that it is set automatically when you log in.",
    "crumbs": [
      "Compute",
      "Apptainer"
    ]
  },
  {
    "objectID": "docs/compute/apptainer.html#using-apptainer",
    "href": "docs/compute/apptainer.html#using-apptainer",
    "title": "Apptainer",
    "section": "",
    "text": "Apptainer containers are designed to be portable, so by default they do not have access to any files on the host system. To make files available to the container, you must bind them to the container. This is done using the --bind option of the apptainer command or the APPTAINER_BINDPATH environment variable.\n\n\n\n\n\n\nDefault bind paths\n\n\n\nBy default, apptainer will make several directories available within the container by binding them to the same path in the container, so that /somefolder on the host is available as /somefolder within the container.\nThese directories include: - $HOME (your home directory, a.k.a. ~) - /tmp (temporary directory ‚Äì unique to each node, contents are purged when the users‚Äô last job running on the node completes) - $PWD (the current working directory, i.e.¬†the directory you are in when you run apptainer)\nThe Klone Apptainer installation is also configured to bind several other following directories to the same path in the container, including:\n\n/mmfs1 (the main filesystem)\n/scr (the scratch filesystem, same as /tmp)\n\n\n\nWe recommend setting the following binds before running a container:\nexport APPTAINER_BINDPATH=\"/gscratch\" # Make /gscratch available within the container\n\nTo start an Apptainer container interactively, run the following:\n\napptainer shell &lt;path_to_container&gt;",
    "crumbs": [
      "Compute",
      "Apptainer"
    ]
  },
  {
    "objectID": "docs/compute/apptainer.html#pulling-an-apptainer-image-from-docker.io-registry",
    "href": "docs/compute/apptainer.html#pulling-an-apptainer-image-from-docker.io-registry",
    "title": "Apptainer",
    "section": "",
    "text": "apptainer pull docker://&lt;image_name&gt;[:&lt;tag&gt;] # Pulls the image from docker registry",
    "crumbs": [
      "Compute",
      "Apptainer"
    ]
  },
  {
    "objectID": "docs/compute/apptainer.html#practical-examples",
    "href": "docs/compute/apptainer.html#practical-examples",
    "title": "Apptainer",
    "section": "",
    "text": "Let‚Äôs try running a Python script using Apptainer. The release notes for Python version 3.11 say that it is ‚Äúbetween 10-60% faster than Python 3.10‚Äù. Is this true? Let‚Äôs find out! We‚Äôll write a simple Python script to test this and use Apptainer to run it on Python 3.10 and Python 3.11.\n\n\nspeedtest.py\n\n#!/usr/bin/env python3\nimport os, sys, timeit\n\n# Get the values of the environment variables M and N, or use default values:\n1n = int(os.getenv(\"N\", default=1000))  # How many numbers to join\nm = int(os.getenv(\"M\", default=100))  # How many times to run the test\n\n# Function to test:\ndef join_nums():\n    return \"-\".join([str(i) for i in range(n)])\n\n2# Print details about the test to stderr:\nprint(\n    f\"Running join_nums() {n}*{m} times on Python v{sys.version_info.major}.{sys.version_info.minor}\",\n    file=sys.stderr,\n)\n\n# Run the test:\n3result = timeit.timeit(join_nums, number=m)\n\n# Print the result:\n4print(result)\n\n\n1\n\nThe os.getenv function retrieves the value of an environment variable or returns a default value if the variable is not set.\n\n2\n\nHere, we use the file parameter to print(), which makes it write to the standard error stream (stderr) instead of the default standard output stream (stdout). This is useful because we want to print the result of the test to stdout so that we can save the output by redirecting it to a file, but we also want to print some information about the test to stderr so that it doesn‚Äôt get mixed up with the result.\n\n3\n\ntimeit.timeit() runs a function multiple times and returns the average time it took to run the function.\n\n4\n\nThe print function prints to the standard output stream (stdout) by default. We can redirect this to a file in bash using the &gt; operator.\n\n\nNow let‚Äôs run the script using Python 3.10 and Python 3.11. For the image, we will use the python:3.10-slim and python:3.11-slim images from the official Python images.\n\n\n\n\n\n\nTagged container releases\n\n\n\nThe python:3.10-slim and python:3.11-slim images are tagged with the version of Python they contain. This means that if you pull the python:3.10-slim image today, it will always contain Python 3.10, even if Python 3.11 is released tomorrow.\nThe python:*-slim images are designed to contain only the minimal set of packages required to run Python and are therefore much smaller than the standard Python images (~45 MiB vs ~350 MiB).\n\n\nWe‚Äôll use the apptainer exec command to run the script inside an Apptainer container:\n1apptainer exec docker://python:3.10-slim python3 speedtest.py\n\n1\n\nThe apptainer exec command runs a command inside an Apptainer container. The docker://python:3.10-slim argument tells Apptainer to use the python:3.10-slim image from the Docker registry. The python3 ./speedtest.py argument tells Apptainer to run the python3 command inside the container and provide it with the argument speedtest.py.\n\n\nYou should see something like:\n1Running join_nums() 1000*100 times on Python v3.11\n20.003154174002702348\n\n1\n\nThis is printed to stderr because we used print(..., file=sys.stderr) in the script.\n\n2\n\nThis is printed to stdout.\n\n\nIt probably didn‚Äôt take very long to run the script. Let‚Äôs try running it again with larger values of M and N. We can set the values of M and N by setting them as environment variables in the bash command prompt before running the script:\n1export M=1000 N=100_000\napptainer exec docker://python:3.10-slim python3 speedtest.py\n\n1\n\nThe export command sets the values of the M and N environment variables and makes them avaialble to other programs like apptainer. We‚Äôre setting N to100_000 instead of 100000 because underscores can be used in Python to make large numbers easier to read. This is a feature of Python, not the shell (which interprets 100_000 as just a string and not a number).\n\n\nIt should take a bit longer to run this time.\nNow let‚Äôs try running the script using Python 3.11:\napptainer exec docker://python:3.11-slim python3 speedtest.py\nWe don‚Äôt need to set the values of M and N again because they are still set from the previous command via export (unless you closed your terminal window or logged out).\nIt probably took a bit less time to run the script this time.\nIs Python 3.11 really faster than Python 3.10? Let‚Äôs find out by redirecting the output of the script to a file:\n1apptainer exec docker://python:3.10-slim python3 speedtest.py &gt; py3.10.txt\napptainer exec docker://python:3.11-slim python3 speedtest.py &gt; py3.11.txt\n2cat py3.10.txt py3.11.txt # show the results\n\n1\n\nIn bash, the &gt; operator redirects the output of a command to a file. If the file already exists, it will be overwritten. If you want to append to an existing file instead, use the &gt;&gt; operator. These operators can be used with any command, not just apptainer. For example, ls &gt; my_files.txt will write the output of ls to the file my_files.txt.\n\n2\n\nThe cat command prints the contents of a file to stdoutIf you want to print the contents of multiple files, you can list them all as arguments tocat`.\n\n\nWe see the results, but they‚Äôre not very easy to interpret. Let‚Äôs pipe the output to Python to calculate the speedup:\n1cat py3.10.txt py3.11.txt | apptainer exec docker://python:3.11-slim python3 -c 'print(float(input())/float(input()))'\n\n1\n\nThe | operator pipes the output of one command to the input of another. In this case, we are piping the output of cat py3.10.txt py3.11.txt to the input of apptainer exec docker://python:3.11-slim python3 -c 'print(float(input())/float(input()))'. The -c option of the python3 command tells Python to run the code provided as an argument. The code print(float(input())/float(input())) reads two lines of input from standard input, converts them to floating point numbers, divides the first by the second, and prints the result.\n\n\nIn my case, Python 3.11 was about 1.3 times faster than Python 3.10. Not bad!\nHere‚Äôs a demo of the above commands. Note that I used M=200 and N=50_000 instead of M=1000 and N=100_000 because it takes a long time to run the script with the larger values of M and N.\n\n\n\n\n\n\nWhat if we wanted to add a command-line interface to make it possible to run the script with different values of M and N without having to set them as environment variables? A user might want to set the values of M and N as command-line arguments. Maybe they would also like to be able to specify the output file instead of redirecting the output to a file. And how about a progress bar? Users love progress bars!\nThis sounds like a tall order, but it‚Äôs actually quite easy to do in Python using the click package for Python. Here‚Äôs the new script:\n\n\nspeedtest-cli.py\n\n#!/usr/bin/env python3\nimport os, sys, timeit\nimport click\nfrom math import sqrt\n\n@click.command()  # Set up the context for the command line interface\n@click.option(\n    \"-n\",  # The name of the option\n    envvar=\"N\",  # Use the environment variable N if it exists\n    default=1000,  # Default value if N is not set\n    help=\"How many numbers to join\",  # Help text for the -n option\n    type=int,  # Convert the value to an integer\n)\n@click.option(\n    \"-m\",  # The name of the option\n    envvar=\"M\",  # Use the environment variable M if it exists\n    default=100,  # Default value if M is not set\n    help=\"How many times to run the test\",  # Help text for the -m option\n    type=int,  # Convert the value to an integer\n)\n@click.option(\n    \"--output\",\n    envvar=\"OUTPUT_FILE\",  # Use the environment variable OUTPUT_FILE if it exists\n    default=\"/dev/stdout\",  # Default value if OUTPUT_FILE is not set\n    help=\"Output file to write the results to\",\n    type=click.Path(writable=True, dir_okay=False),\n)\ndef speedtest(m, n, output):\n    \"\"\"Run a speed test.\"\"\"  # Help text for the command\n\n    bar = click.progressbar(length=n * m,\n        update_min_steps=sqrt(n * m), # How often to update the progress bar\n        label=\"Joining numbers\",\n        file=sys.stderr,\n    )\n\n    # Function to test:\n    def join_nums():\n        result = \"-\".join([str(i) for i in range(n)])\n        bar.update(n)\n        return result\n\n    # Print details about the test to stderr:\n    print(\n        f\"Running join_nums() {n}*{m} times on Python v{sys.version_info.major}.{sys.version_info.minor}\",\n        file=sys.stderr,\n    )\n    # Run the test:\n    result = timeit.timeit(join_nums, number=m)\n    bar.render_finish()\n\n    # Print the result:\n    with open(output, \"w\") as f:  # Open the output file for writing\n        print(result, file=f)\n    print(f\"Result written to {output}\", file=sys.stderr)\n\n# Run the command:\nspeedtest()\n\nNow, let‚Äôs try running the script with apptainer exec:\napptainer exec docker://python:3.11-slim python3 speedtest-cli.py\nYou probaby got a ModuleNotFoundError because the click package is not installed in the python:3.11-slim image. If you know some Python, you might think, ‚ÄúI could probably fix this if I install the click module via pip.‚Äù And you would be right ‚Äì if you were running the script in the version of Python that‚Äôs normally installed on your computer. But we‚Äôre running the script inside an Apptainer container, so we need to install the click module inside the container. Furthermore, we‚Äôre using two different versions of Python, so we need to install the click module in the containers for both versions.\nWe‚Äôll solve this by writing an Apptainer definition file to build a custom Apptainer image that contains the click module. Here‚Äôs the definition file:\n\n\nspeedtest.def\n\nBootstrap: docker # Specify where to get the container from.\nFrom: python:{{PY_VERSION}} # Which container to use as a base.\n\n# Define the arguments that can be passed at `apptainer build` time:\n%arguments\n    # This replaces {{PY_VERSION}} in the `From:` line above:\n    PY_VERSION=3.10\n  \n%files\n    # Copy the Python script to the container as the command `speedtest`:\n    speedtest-cli.py /opt/local/bin/speedtest\n    \n%environment\n    # Set the virtual environment as the default Python environment:\n    export PATH=\"/opt/venv/bin:$PATH\" # Add the virtual environment to the PATH.\n    export PATH=\"/opt/local/bin:$PATH\" # Add the directory with the `speedtest` command to the PATH.\n\n%post\n    # Create a virtual environment in /opt/venv to install our dependencies:\n    python -m venv /opt/venv\n\n    # Set the virtual environment as the default Python environment for the rest of the script:\n    # (the %environment section does not apply to the %post section)\n    export PATH=\"/opt/venv/bin:$PATH\"\n\n    # Install `click` and don't cache the downloaded files:\n    pip install --no-cache-dir click\n\n    # Print a message to stderr to let the user know that the installation is done:\n    echo \"$(python --version): Done installing dependencies.\" &gt;&2\n    # (python --version prints the result of the command `python --version`.)\n    # (The &gt;&2 part redirects the output to stderr instead of stdout.)\n\n    chmod +x /opt/local/bin/speedtest # Make the `speedtest` command executable.\n\n%runscript\n    # Run the Python script with the arguments passed to the container:\n    speedtest \"$@\"\n\nNow we can build the image:\napptainer build speedtest-py3.10.sif speedtest.def\nAnd now we can run the script using the new image with the apptainer run commanad, which is similar to apptainer exec but runs the command specified in the definition file instead of the python command specified as an argument:\napptainer run speedtest-py3.10.sif\nTry it with some different values of M and N:\napptainer run speedtest-py3.10.sif -m 30_000 -n 1000\nNow try specifying a different output file:\napptainer run speedtest-py3.10.sif -- -m 30_000 -n 1000 --output py3.10.txt\ncat py3.10.txt # Show the results\nHow do we build the image for Python 3.11? We could copy the definition file and change PY_VERSION to 3.11, but that would be a lot of work. Instead, we can use the --build-arg option of the apptainer build command to pass the value of PY_VERSION as a build argument to the definition file:\napptainer build --build-arg PY_VERSION=3.11 speedtest-py3.11.sif speedtest.def\nNow we can run the script using the new image:\napptainer run speedtest-py3.11.sif -m 30_000 -n 1000\nWe can still use the environment variables M and N to set the values of m and n:\nexport M=30_000 N=1000\napptainer run speedtest-py3.11.sif\nThis makes it easier to run both containers with the same values of M and N without having to specify them each time.\nIf we wanted to, we could even use OUTPUT_FILE to specify the output file instead of using the --output option, because we let click know that OUTPUT_FILE is an alternative for the --output argument if there is no --output option:\nexport OUTPUT_FILE=py3.11-2.txt\napptainer run speedtest-py3.11.sif\nunset OUTPUT_FILE # Unset the OUTPUT_FILE environment variable so that it doesn't affect the next command\nBecause we specified the %runscript section in the definition file, we can also execute the script directly without having to specify the apptainer command:\n./speedtest-py3.11.sif -m 30_000 -n 1000\nLet‚Äôs recreate the comparison we did earlier:\nexport M=1000 N=100_000\napptainer run speedtest-py3.10.sif --output py3.10.txt\napptainer run speedtest-py3.11.sif --output py3.11.txt\n1cat py3.10.txt py3.11.txt | apptainer exec speedtest-py3.10.sif python3 -c 'print(float(input())/float(input()))'\n\n1\n\nWe‚Äôre using apptainer exec instead of apptainer run because we want to run the python3 command instead of the command specified in the definition file.\n\n\n\n\n\n\nApptainer caches all the images it downloads in a cache directory to avoid downloading them again. The cache can get quite large, so it‚Äôs a good idea to clear it from time to time.\nYou can see the size of the cache directory by running:\napptainer cache list\nTo clear the cache, run:\napptainer cache clean\nThat should free up some space.\n\n\n\nBy default, Apptainer stores the cache in ~/.apptainer/cache. This can be a problem if you have a small home directory (e.g.¬†if you are using the default 10 GB quota on Klone). You can change the cache directory by setting the APPTAINER_CACHE environment variable. For example, to set the cache directory to /tmp/&lt;your-username&gt;/apptainer-cache, you can use the $USER environment variable:\nexport APPTAINER_CACHE=\"/tmp/$USER/apptainer-cache\"\nApptainer will create the cache directory if it does not already exist.\nYou‚Äôll need to set the APPTAINER_CACHE environment variable every time you want to use Apptainer, so it‚Äôs a good idea to add it to your ~/.bashrc file so that it is set automatically when you log in.",
    "crumbs": [
      "Compute",
      "Apptainer"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html",
    "href": "docs/compute/lmod.html",
    "title": "Lmod",
    "section": "",
    "text": "Lmod is a module system that is used to load additional software on Klone compute nodes.\nLmod is run with the command module. This command is only available on compute nodes and will not run on the login node, so you need to launch a SLURM job on Klone to run the command.\n\n\nmodule help\n\n\n\nmodule avail\n\n\n\nmodule load &lt;module_name&gt; # To load the default version of a module\nmodule load &lt;module_name&gt;/&lt;version&gt; # To load a specific version of a module\nIf the version is not specified, Lmod will load a default version marked with D.\n\n\n\nmodule help &lt;module&gt;\n\n\n\nmodule list\n\n\n\nmodule unload &lt;module&gt;\n\n\n\nmodule purge\n\n\n\nmodule whatis &lt;module&gt;",
    "crumbs": [
      "Compute",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#show-help-for-lmod",
    "href": "docs/compute/lmod.html#show-help-for-lmod",
    "title": "Lmod",
    "section": "",
    "text": "module help",
    "crumbs": [
      "Compute",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#checking-all-available-modules",
    "href": "docs/compute/lmod.html#checking-all-available-modules",
    "title": "Lmod",
    "section": "",
    "text": "module avail",
    "crumbs": [
      "Compute",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#load-a-module",
    "href": "docs/compute/lmod.html#load-a-module",
    "title": "Lmod",
    "section": "",
    "text": "module load &lt;module_name&gt; # To load the default version of a module\nmodule load &lt;module_name&gt;/&lt;version&gt; # To load a specific version of a module\nIf the version is not specified, Lmod will load a default version marked with D.",
    "crumbs": [
      "Compute",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#show-help-for-a-module",
    "href": "docs/compute/lmod.html#show-help-for-a-module",
    "title": "Lmod",
    "section": "",
    "text": "module help &lt;module&gt;",
    "crumbs": [
      "Compute",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#list-loaded-modules",
    "href": "docs/compute/lmod.html#list-loaded-modules",
    "title": "Lmod",
    "section": "",
    "text": "module list",
    "crumbs": [
      "Compute",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#unload-a-module",
    "href": "docs/compute/lmod.html#unload-a-module",
    "title": "Lmod",
    "section": "",
    "text": "module unload &lt;module&gt;",
    "crumbs": [
      "Compute",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#unload-all-modules",
    "href": "docs/compute/lmod.html#unload-all-modules",
    "title": "Lmod",
    "section": "",
    "text": "module purge",
    "crumbs": [
      "Compute",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/compute/lmod.html#show-module-description",
    "href": "docs/compute/lmod.html#show-module-description",
    "title": "Lmod",
    "section": "",
    "text": "module whatis &lt;module&gt;",
    "crumbs": [
      "Compute",
      "Lmod"
    ]
  },
  {
    "objectID": "docs/software/afni.html",
    "href": "docs/software/afni.html",
    "title": "AFNI",
    "section": "",
    "text": "module avail afni\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCurrently, AFNI only runs in a VNC Apptainer environment with sufficient dependencies.\n\n\nLoad AFNI with module load forsyth/afni.\nAfter loading the module, all AFNI tools are made accessible in the shell session.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is provided as reference material.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is ran from a VNC Apptainer environment with sufficient dependencies.\n\n\nTODO: provide steps to prepare Apptainer environment to run installer.\n\nDownload AFNI installer:\n\ncurl -O https://afni.nimh.nih.gov/pub/dist/bin/misc/@update.afni.binaries\n\nInstall the CentOS 7 package to /sw/contrib/labname-src/afni:\n\nexport LABNAME=escience\nexport TARGET=/sw/contrib/\"$LABNAME\"-src/afni/\ntcsh @update.afni.binaries -package linux_centos_7_64 -do_extras -bindir $TARGET\n\nInstall required R libraries to /sw/contrib/labname-src/afni/Rlibs:\n\nexport R_LIBS=$TARGET/Rlibs\nmkdir -p $R_LIBS\nrPkgsInstall -pkgs ALL\n\nCreate an Lmod .lua module file for AFNI with a text editor:\n\n\n\n\n\n\n\nNote\n\n\n\n/sw/contrib/modulefiles/labname/afni.lua ^^^\nhelp([[afni]])\n\nlocal labname = \"escience\"\nlocal base = \"/sw/contrib/\" .. labname .. \"-src/afni\"\nlocal r_libs = pathJoin(base, \"Rlibs\")\n\nappend_path(\"PATH\", base)\nsetenv(\"R_LIBS\", r_libs)\n\nif (mode() == \"load\") then\n    LmodMessage(\"R_LIBS set to \" .. r_libs)\n    LmodMessage(\"------------------------------------------------------------\")\n    LmodMessage(\"On initial use, run the following:\")\n    LmodMessage(\"  suma -update_env\")\nend\n\nwhatis(\"Name: \" .. name)\nwhatis(\"Version: \" .. version)\n\n\n\n\n\n\n\n\nNote\n\n\n\nLmod takes some time to cache available modules. If a module does not appear, use the -I or --ignore_cache flag to force Lmod to check for new modules.\nmodule -I avail afni\nmodule -I load labname/afni",
    "crumbs": [
      "Software",
      "AFNI"
    ]
  },
  {
    "objectID": "docs/software/afni.html#checking-available-versions",
    "href": "docs/software/afni.html#checking-available-versions",
    "title": "AFNI",
    "section": "",
    "text": "module avail afni",
    "crumbs": [
      "Software",
      "AFNI"
    ]
  },
  {
    "objectID": "docs/software/afni.html#using-afni-from-forsyth",
    "href": "docs/software/afni.html#using-afni-from-forsyth",
    "title": "AFNI",
    "section": "",
    "text": "Note\n\n\n\nCurrently, AFNI only runs in a VNC Apptainer environment with sufficient dependencies.\n\n\nLoad AFNI with module load forsyth/afni.\nAfter loading the module, all AFNI tools are made accessible in the shell session.",
    "crumbs": [
      "Software",
      "AFNI"
    ]
  },
  {
    "objectID": "docs/software/afni.html#installing-afni",
    "href": "docs/software/afni.html#installing-afni",
    "title": "AFNI",
    "section": "",
    "text": "Note\n\n\n\nThis is provided as reference material.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is ran from a VNC Apptainer environment with sufficient dependencies.\n\n\nTODO: provide steps to prepare Apptainer environment to run installer.\n\nDownload AFNI installer:\n\ncurl -O https://afni.nimh.nih.gov/pub/dist/bin/misc/@update.afni.binaries\n\nInstall the CentOS 7 package to /sw/contrib/labname-src/afni:\n\nexport LABNAME=escience\nexport TARGET=/sw/contrib/\"$LABNAME\"-src/afni/\ntcsh @update.afni.binaries -package linux_centos_7_64 -do_extras -bindir $TARGET\n\nInstall required R libraries to /sw/contrib/labname-src/afni/Rlibs:\n\nexport R_LIBS=$TARGET/Rlibs\nmkdir -p $R_LIBS\nrPkgsInstall -pkgs ALL\n\nCreate an Lmod .lua module file for AFNI with a text editor:\n\n\n\n\n\n\n\nNote\n\n\n\n/sw/contrib/modulefiles/labname/afni.lua ^^^\nhelp([[afni]])\n\nlocal labname = \"escience\"\nlocal base = \"/sw/contrib/\" .. labname .. \"-src/afni\"\nlocal r_libs = pathJoin(base, \"Rlibs\")\n\nappend_path(\"PATH\", base)\nsetenv(\"R_LIBS\", r_libs)\n\nif (mode() == \"load\") then\n    LmodMessage(\"R_LIBS set to \" .. r_libs)\n    LmodMessage(\"------------------------------------------------------------\")\n    LmodMessage(\"On initial use, run the following:\")\n    LmodMessage(\"  suma -update_env\")\nend\n\nwhatis(\"Name: \" .. name)\nwhatis(\"Version: \" .. version)\n\n\n\n\n\n\n\n\nNote\n\n\n\nLmod takes some time to cache available modules. If a module does not appear, use the -I or --ignore_cache flag to force Lmod to check for new modules.\nmodule -I avail afni\nmodule -I load labname/afni",
    "crumbs": [
      "Software",
      "AFNI"
    ]
  },
  {
    "objectID": "docs/software/freesurfer.html",
    "href": "docs/software/freesurfer.html",
    "title": "FreeSurfer",
    "section": "",
    "text": "Important\n\n\n\nCurrently, FreeSurfer must run from a container with sufficient library dependencies.\n\n\n\n\n\n\nTip\n\n\n\nWe recommend using FreeSurfer from a VNC session with rockylinux8 container from https://github.com/uw-psych/hyak_vnc_apptainer\n\n\n\n\nFreeSurfer is an open source neuroimaging toolkit and is available as an Lmod module on Klone.\n\n\nmodule avail freesurfer\n\n\n\nLoad the default version with module load escience/freesurfer,\nor load a specific version with module load escience/freesurfer/&lt;version&gt;.\nAfter loading the module, FreeSurfer should have its environment setup for normal use.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is provided as reference material.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is tested from a VNC Apptainer environment with sufficient dependencies.\n\n\nTODO: provide steps to prepare Apptainer environment to run installer.\n\nGo to https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall and download the latest FreeSurfer tar archive for CentOS 8\nExtract the .tar.gz archive to /sw/contrib/labname-src/freesurfer/&lt;version&gt;/.\n\nFor example, if installing version 7.3.2 of FreeSurfer, then run the following:\nexport LABNAME=escience\nexport VERSION=7.3.2\nmkdir -p /sw/contrib/\"$LABNAME\"-src/freesurfer/\"$VERSION\"\ntar -xzf freesurfer-linux-centos8_x86_64-\"$VERSION\".tar.gz \\\n    --strip-components=1 \\\n    -C /sw/contrib/\"$LABNAME\"-src/freesurfer/\"$VERSION\"\n\nCreate an Lmod .lua module file for the new release with a text editor:\n\n\n\n\n\n\n\nNote\n\n\n\n/sw/contrib/modulefiles/labname/freesurfer/7.3.2.lua ^^^\nhelp([[freesurfer-7.3.2]])\n\nlocal labname = \"escience\"\nlocal version = \"7.3.2\"\nlocal base = pathJoin(\"/sw/contrib/\" .. labname .. \"-src/freesurfer/\", version)\n\n--FreeSurfer 7.x requires MATLAB R2014b\ndepend_on(\"forsyth/matlab/r2014b\")\n\nsetenv(\"FREESURFER_HOME\", base)\nprepend_path(\"PATH\", pathJoin(base, \"bin\"))\nsource_sh(\"bash\", pathJoin(base, \"SetUpFreeSurfer.sh\"))\nwhatis(\"Name: FreeSurfer\")\nwhatis(\"Version: \" .. version)\n\n\n\n\n\n\n\n\nNote\n\n\n\nLmod takes some time to cache available modules. If a module is not appearing, use the -I or --ignore_cache flag to force Lmod to check for new modules.\nmodule -I avail freesurfer\nmodule -I load labname/freesurfer\n\n\n\nRequest a free license from https://surfer.nmr.mgh.harvard.edu/registration.html if not already.\n\n\n\n\n\n\n\nNote\n\n\n\nThis license is not tied to a specific version of FreeSurfer and can be copied from a previous installation.\n\n\n\nDownload license.txt from the registration email and copy it to /sw/contrib/labname-src/freesurfer/&lt;version&gt;/license.txt.\nFor FS-FAST support, create a link to a supported MATLAB release (R2014b for FreeSurfer 7.x):\n\nmodule -I load labname/freesurfer/&lt;version&gt;\ncd $FREESURFER_HOME\nln -s /gscratch/forsyth-src/matlab/R2014b MCRv84\n\n\n\n\n\n\nNote\n\n\n\nLmod takes some time to cache available modules. If a module does not appear, use the -I or --ignore_cache flag to force Lmod to check for new modules.\nmodule -I avail freesurfer\nmodule -I load labname/freesurfer",
    "crumbs": [
      "Software",
      "FreeSurfer"
    ]
  },
  {
    "objectID": "docs/software/freesurfer.html#checking-available-versions",
    "href": "docs/software/freesurfer.html#checking-available-versions",
    "title": "FreeSurfer",
    "section": "",
    "text": "module avail freesurfer",
    "crumbs": [
      "Software",
      "FreeSurfer"
    ]
  },
  {
    "objectID": "docs/software/freesurfer.html#using-freesurfer-from-escience",
    "href": "docs/software/freesurfer.html#using-freesurfer-from-escience",
    "title": "FreeSurfer",
    "section": "",
    "text": "Load the default version with module load escience/freesurfer,\nor load a specific version with module load escience/freesurfer/&lt;version&gt;.\nAfter loading the module, FreeSurfer should have its environment setup for normal use.",
    "crumbs": [
      "Software",
      "FreeSurfer"
    ]
  },
  {
    "objectID": "docs/software/freesurfer.html#installing-a-different-version",
    "href": "docs/software/freesurfer.html#installing-a-different-version",
    "title": "FreeSurfer",
    "section": "",
    "text": "Note\n\n\n\nThis is provided as reference material.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is tested from a VNC Apptainer environment with sufficient dependencies.\n\n\nTODO: provide steps to prepare Apptainer environment to run installer.\n\nGo to https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall and download the latest FreeSurfer tar archive for CentOS 8\nExtract the .tar.gz archive to /sw/contrib/labname-src/freesurfer/&lt;version&gt;/.\n\nFor example, if installing version 7.3.2 of FreeSurfer, then run the following:\nexport LABNAME=escience\nexport VERSION=7.3.2\nmkdir -p /sw/contrib/\"$LABNAME\"-src/freesurfer/\"$VERSION\"\ntar -xzf freesurfer-linux-centos8_x86_64-\"$VERSION\".tar.gz \\\n    --strip-components=1 \\\n    -C /sw/contrib/\"$LABNAME\"-src/freesurfer/\"$VERSION\"\n\nCreate an Lmod .lua module file for the new release with a text editor:\n\n\n\n\n\n\n\nNote\n\n\n\n/sw/contrib/modulefiles/labname/freesurfer/7.3.2.lua ^^^\nhelp([[freesurfer-7.3.2]])\n\nlocal labname = \"escience\"\nlocal version = \"7.3.2\"\nlocal base = pathJoin(\"/sw/contrib/\" .. labname .. \"-src/freesurfer/\", version)\n\n--FreeSurfer 7.x requires MATLAB R2014b\ndepend_on(\"forsyth/matlab/r2014b\")\n\nsetenv(\"FREESURFER_HOME\", base)\nprepend_path(\"PATH\", pathJoin(base, \"bin\"))\nsource_sh(\"bash\", pathJoin(base, \"SetUpFreeSurfer.sh\"))\nwhatis(\"Name: FreeSurfer\")\nwhatis(\"Version: \" .. version)\n\n\n\n\n\n\n\n\nNote\n\n\n\nLmod takes some time to cache available modules. If a module is not appearing, use the -I or --ignore_cache flag to force Lmod to check for new modules.\nmodule -I avail freesurfer\nmodule -I load labname/freesurfer\n\n\n\nRequest a free license from https://surfer.nmr.mgh.harvard.edu/registration.html if not already.\n\n\n\n\n\n\n\nNote\n\n\n\nThis license is not tied to a specific version of FreeSurfer and can be copied from a previous installation.\n\n\n\nDownload license.txt from the registration email and copy it to /sw/contrib/labname-src/freesurfer/&lt;version&gt;/license.txt.\nFor FS-FAST support, create a link to a supported MATLAB release (R2014b for FreeSurfer 7.x):\n\nmodule -I load labname/freesurfer/&lt;version&gt;\ncd $FREESURFER_HOME\nln -s /gscratch/forsyth-src/matlab/R2014b MCRv84\n\n\n\n\n\n\nNote\n\n\n\nLmod takes some time to cache available modules. If a module does not appear, use the -I or --ignore_cache flag to force Lmod to check for new modules.\nmodule -I avail freesurfer\nmodule -I load labname/freesurfer",
    "crumbs": [
      "Software",
      "FreeSurfer"
    ]
  },
  {
    "objectID": "docs/software/spm_conn.html",
    "href": "docs/software/spm_conn.html",
    "title": "SPM and CONN Toolboxes",
    "section": "",
    "text": "Statistical Parametric Mapping (SPM) and CONN are available as Lmod modules on Klone compute nodes &gt;}}\n\n\nLike all Lmod modules, the spm and conn modules are available only on Klone compute nodes, and these commands will not work on the login node.\n\n\nBecause the modules for SPM and CONN are installed in the escience hierarchy, their names are prefixed with escience/.\nYou can check the installed versions as follows:\nmodule spider escience/spm escience/conn\nThe output should look a bit like this:\n---------------------------------------\n  escience/conn: escience/conn/v.22.a\n---------------------------------------\n\n    This module can be loaded directly: module load escience/conn/v.22.a\n\n    Help:\n      conn-v.22.a\n\n---------------------------------------\n  escience/spm: escience/spm/12\n---------------------------------------\n\n    This module can be loaded directly: module load escience/spm/12\n\n    Help:\n      spm12\n\n\n\nLoad the default version with module load escience/spm or module load escience/conn, or load a specific version with module load escience/spm/&lt;version&gt; or module load escience/conn/&lt;version&gt;.\nAfter loading the module, start matlab and run spm or conn in the MATLAB command window.\n\n\n\nYou can install different versions of SPM and CONN by following the instructions below.\n\n\n\nGo to https://www.fil.ion.ucl.ac.uk/spm/software/download, choose the version of SPM you want to install, copy the download link to the file (with the extension .zip), and download the file by running the following command on a Klone login or compute node. We‚Äôll use version 8 as an example:\n1wget -P \"/gscratch/scrubbed/$USER/downloads\" https://www.fil.ion.ucl.ac.uk/spm/download/restricted/idyll/spm8.zip\n\n1\n\nwget is a program you can use to download files from the web. Replace the URL with the URL for the version of SPM you want to install. The -P flag tells wget to save the file to the specified directory. The directory /gscratch/scrubbed/$USER/downloads is a good place to save files temporarily ‚Äì see our guide to storage on Klone for more information.\n\n\nExtract the .zip archive to /sw/contrib/mylabname-src/spm (replacing mylabname with the name of your lab).\n# Be sure to replace \"mylabname\" with the name of your lab!\n1LABNAME=mylabname\n2mkdir -p \"/sw/contrib/${LABNAME}-src/spm\"\nunzip -n \"/gscratch/scrubbed/$USER/downloads/spm8.zip\" -d \"/sw/contrib/${LABNAME}-src/spm\"\n\n1\n\nCreate the target directory if it does not exist.\n\n\n2\n\nExtract the SPM archive to the target directory (The -n flag prevents overwriting existing files if someone has already installed this version of SPM.).\n\n\nCreate an Lmod .lua module file for the new release with a text editor, using &lt;version&gt;.lua as the filename:\n\n\n/sw/contrib/modulefiles/mylabname/spm/8.lua\n\nhelp([[spm8]])\n -- Be sure to replace \"mylabname\" with the name of your lab!\nlocal labname = \"mylabname\"\nlocal version = \"8\"\nlocal base = \"/sw/contrib/\" .. labname .. \"-src/spm/spm\" .. version\ndepends_on(\"matlab\")\nappend_path(\"MATLABPATH\", base)\nwhatis(\"Name: spm\")\nwhatis(\"Version: \" .. version)\n\nRemove the downloaded file when you are done with it:\nrm -v \"/gscratch/scrubbed/$USER/downloads/spm8.zip\"\n\n\n\n\n\nGo to https://www.nitrc.org/frs/?group_id=279, choose the version of CONN you want to install, copy the download link to the file (with the extension .zip), and download the file by running the following command on a Klone login or compute node. We‚Äôll use version 21a as an example:\n1 wget -P \"/gscratch/scrubbed/$USER/downloads\" https://www.nitrc.org/frs/download.php/12426/conn21a.zip\n\n1\n\nwget is a program you can use to download files from the web. Replace the URL with the URL for the version of SPM you want to install. The -P flag tells wget to save the file to the specified directory. The directory /gscratch/scrubbed/$USER/downloads is a good place to save files temporarily ‚Äì see our guide to storage on Klone for more information.\n\n\nExtract the .zip archive to /sw/contrib/mylabname-src/spm (replacing mylabname with the name of your lab).\n# Be sure to replace \"mylabname\" with the name of your lab!\n1LABNAME=mylabname\n2mkdir -p \"/sw/contrib/${LABNAME}-src/conn\"\nunzip -n \"/gscratch/scrubbed/$USER/downloads/conn21a.zip\" -d \"/sw/contrib/${LABNAME}-src/conn/v.21.a\"\n\n1\n\nCreate the target directory if it does not exist.\n\n\n2\n\nExtract the SPM archive to directory v.21.a within the target directory. (The -n flag prevents overwriting existing files if someone has already installed this version of CONN.)\n\n\nCreate an Lmod .lua module file for the new release with a text editor, using &lt;version&gt;.lua as the filename:\n\n\n/sw/contrib/modulefiles/mylabname/conn/v.21.a.lua\n\n-- Be sure to replace \"mylabname\" with the name of your lab!\nhelp([[conn-v.21.a]])\nlocal labname = \"mylabname\"\nlocal version = \"v.21.a\"\nlocal base = pathJoin(\"/sw/contrib/\" .. labname .. \"-src/conn\", version, \"conn\")\ndepends_on(\"matlab\")\ndepends_on(\"escience/spm\")\nappend_path(\"MATLABPATH\", base)\nwhatis(\"Name: CONN\")\nwhatis(\"Version: \" .. version)\n\nRemove the downloaded file when you are done with it:\nrm -v \"/gscratch/scrubbed/$USER/downloads/conn21a.zip\"\n\n\n\n\n\n\n\nNote\n\n\n\nLmod takes some time to cache available modules. If a module does not appear, use the -I or --ignore_cache flag to force Lmod to check for new modules.\nmodule -I load mylabname/spm",
    "crumbs": [
      "Software",
      "SPM and CONN Toolboxes"
    ]
  },
  {
    "objectID": "docs/software/spm_conn.html#use-with-lmod",
    "href": "docs/software/spm_conn.html#use-with-lmod",
    "title": "SPM and CONN Toolboxes",
    "section": "",
    "text": "Like all Lmod modules, the spm and conn modules are available only on Klone compute nodes, and these commands will not work on the login node.\n\n\nBecause the modules for SPM and CONN are installed in the escience hierarchy, their names are prefixed with escience/.\nYou can check the installed versions as follows:\nmodule spider escience/spm escience/conn\nThe output should look a bit like this:\n---------------------------------------\n  escience/conn: escience/conn/v.22.a\n---------------------------------------\n\n    This module can be loaded directly: module load escience/conn/v.22.a\n\n    Help:\n      conn-v.22.a\n\n---------------------------------------\n  escience/spm: escience/spm/12\n---------------------------------------\n\n    This module can be loaded directly: module load escience/spm/12\n\n    Help:\n      spm12\n\n\n\nLoad the default version with module load escience/spm or module load escience/conn, or load a specific version with module load escience/spm/&lt;version&gt; or module load escience/conn/&lt;version&gt;.\nAfter loading the module, start matlab and run spm or conn in the MATLAB command window.\n\n\n\nYou can install different versions of SPM and CONN by following the instructions below.\n\n\n\nGo to https://www.fil.ion.ucl.ac.uk/spm/software/download, choose the version of SPM you want to install, copy the download link to the file (with the extension .zip), and download the file by running the following command on a Klone login or compute node. We‚Äôll use version 8 as an example:\n1wget -P \"/gscratch/scrubbed/$USER/downloads\" https://www.fil.ion.ucl.ac.uk/spm/download/restricted/idyll/spm8.zip\n\n1\n\nwget is a program you can use to download files from the web. Replace the URL with the URL for the version of SPM you want to install. The -P flag tells wget to save the file to the specified directory. The directory /gscratch/scrubbed/$USER/downloads is a good place to save files temporarily ‚Äì see our guide to storage on Klone for more information.\n\n\nExtract the .zip archive to /sw/contrib/mylabname-src/spm (replacing mylabname with the name of your lab).\n# Be sure to replace \"mylabname\" with the name of your lab!\n1LABNAME=mylabname\n2mkdir -p \"/sw/contrib/${LABNAME}-src/spm\"\nunzip -n \"/gscratch/scrubbed/$USER/downloads/spm8.zip\" -d \"/sw/contrib/${LABNAME}-src/spm\"\n\n1\n\nCreate the target directory if it does not exist.\n\n\n2\n\nExtract the SPM archive to the target directory (The -n flag prevents overwriting existing files if someone has already installed this version of SPM.).\n\n\nCreate an Lmod .lua module file for the new release with a text editor, using &lt;version&gt;.lua as the filename:\n\n\n/sw/contrib/modulefiles/mylabname/spm/8.lua\n\nhelp([[spm8]])\n -- Be sure to replace \"mylabname\" with the name of your lab!\nlocal labname = \"mylabname\"\nlocal version = \"8\"\nlocal base = \"/sw/contrib/\" .. labname .. \"-src/spm/spm\" .. version\ndepends_on(\"matlab\")\nappend_path(\"MATLABPATH\", base)\nwhatis(\"Name: spm\")\nwhatis(\"Version: \" .. version)\n\nRemove the downloaded file when you are done with it:\nrm -v \"/gscratch/scrubbed/$USER/downloads/spm8.zip\"\n\n\n\n\n\nGo to https://www.nitrc.org/frs/?group_id=279, choose the version of CONN you want to install, copy the download link to the file (with the extension .zip), and download the file by running the following command on a Klone login or compute node. We‚Äôll use version 21a as an example:\n1 wget -P \"/gscratch/scrubbed/$USER/downloads\" https://www.nitrc.org/frs/download.php/12426/conn21a.zip\n\n1\n\nwget is a program you can use to download files from the web. Replace the URL with the URL for the version of SPM you want to install. The -P flag tells wget to save the file to the specified directory. The directory /gscratch/scrubbed/$USER/downloads is a good place to save files temporarily ‚Äì see our guide to storage on Klone for more information.\n\n\nExtract the .zip archive to /sw/contrib/mylabname-src/spm (replacing mylabname with the name of your lab).\n# Be sure to replace \"mylabname\" with the name of your lab!\n1LABNAME=mylabname\n2mkdir -p \"/sw/contrib/${LABNAME}-src/conn\"\nunzip -n \"/gscratch/scrubbed/$USER/downloads/conn21a.zip\" -d \"/sw/contrib/${LABNAME}-src/conn/v.21.a\"\n\n1\n\nCreate the target directory if it does not exist.\n\n\n2\n\nExtract the SPM archive to directory v.21.a within the target directory. (The -n flag prevents overwriting existing files if someone has already installed this version of CONN.)\n\n\nCreate an Lmod .lua module file for the new release with a text editor, using &lt;version&gt;.lua as the filename:\n\n\n/sw/contrib/modulefiles/mylabname/conn/v.21.a.lua\n\n-- Be sure to replace \"mylabname\" with the name of your lab!\nhelp([[conn-v.21.a]])\nlocal labname = \"mylabname\"\nlocal version = \"v.21.a\"\nlocal base = pathJoin(\"/sw/contrib/\" .. labname .. \"-src/conn\", version, \"conn\")\ndepends_on(\"matlab\")\ndepends_on(\"escience/spm\")\nappend_path(\"MATLABPATH\", base)\nwhatis(\"Name: CONN\")\nwhatis(\"Version: \" .. version)\n\nRemove the downloaded file when you are done with it:\nrm -v \"/gscratch/scrubbed/$USER/downloads/conn21a.zip\"\n\n\n\n\n\n\n\nNote\n\n\n\nLmod takes some time to cache available modules. If a module does not appear, use the -I or --ignore_cache flag to force Lmod to check for new modules.\nmodule -I load mylabname/spm",
    "crumbs": [
      "Software",
      "SPM and CONN Toolboxes"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Compute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started\n\n\n\n\n\nGetting started with the Hyak cluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/software/index.html",
    "href": "docs/software/index.html",
    "title": "Software",
    "section": "",
    "text": "This section will cover modules maintained by labs within the Psychology Department including steps for preparing new modules.",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "docs/software/index.html#overview",
    "href": "docs/software/index.html#overview",
    "title": "Software",
    "section": "Overview",
    "text": "Overview",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "docs/software/matlab.html",
    "href": "docs/software/matlab.html",
    "title": "MATLAB",
    "section": "",
    "text": "module avail matlab\n\n\n\nLoad the default version with module load matlab,\nor load a specific version with module load matlab/&lt;version&gt;.\n\n\n\nTo run Matlab with GUI (in a VNC session or with X11-Forwarding enabled), run:\nmatlab\nTo run Matlab from terminal CLI, run:\nmatlab -nodisplay\n\n\n\nTODO",
    "crumbs": [
      "Software",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/software/matlab.html#checking-available-versions",
    "href": "docs/software/matlab.html#checking-available-versions",
    "title": "MATLAB",
    "section": "",
    "text": "module avail matlab",
    "crumbs": [
      "Software",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/software/matlab.html#loading-matlab",
    "href": "docs/software/matlab.html#loading-matlab",
    "title": "MATLAB",
    "section": "",
    "text": "Load the default version with module load matlab,\nor load a specific version with module load matlab/&lt;version&gt;.",
    "crumbs": [
      "Software",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/software/matlab.html#running-matlab",
    "href": "docs/software/matlab.html#running-matlab",
    "title": "MATLAB",
    "section": "",
    "text": "To run Matlab with GUI (in a VNC session or with X11-Forwarding enabled), run:\nmatlab\nTo run Matlab from terminal CLI, run:\nmatlab -nodisplay",
    "crumbs": [
      "Software",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/software/matlab.html#installing-a-different-version",
    "href": "docs/software/matlab.html#installing-a-different-version",
    "title": "MATLAB",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "Software",
      "MATLAB"
    ]
  },
  {
    "objectID": "docs/compute/index.html",
    "href": "docs/compute/index.html",
    "title": "Compute",
    "section": "",
    "text": "UW Hyak clusters use Slurm to manage access to compute resources and schedule compute jobs.",
    "crumbs": [
      "Compute"
    ]
  },
  {
    "objectID": "docs/compute/index.html#overview",
    "href": "docs/compute/index.html#overview",
    "title": "Compute",
    "section": "Overview",
    "text": "Overview",
    "crumbs": [
      "Compute"
    ]
  },
  {
    "objectID": "docs/compute/slurm.html",
    "href": "docs/compute/slurm.html",
    "title": "SLURM Basics",
    "section": "",
    "text": "The Klone cluster uses the SLURM job scheduler to manage access to compute resources and schedule user-submitted jobs.\nUsers can submit a batch job for scripted parallel tasks or an interactive job for manual tasks or GUI programs.\n\n\nUse groups command to see which groups you are a member of.\nUse hyakalloc command to check availability of compute resources.\n\n\n\nsalloc -A &lt;lab&gt; -p &lt;node_type&gt; -N &lt;NUM_NODES&gt; -c &lt;NUM_CPUS&gt; --mem=&lt;MEM&gt;[UNIT] --time=DD-HH:MM:SS\nAdd --no-shell flag if you plan to enter/exit the node without losing the job allocation.\nUse scancel &lt;job_id&gt; to terminate the job allocation.\nUse squeue --me to check information about active or pending job allocations (including job ID).\n\n\n\nFrom the login node, run the following:\nsrun -A &lt;lab&gt; -p &lt;node_type&gt; -N &lt;NUM_NODES&gt; -c &lt;NUM_CPUS&gt; --mem=&lt;MEM&gt;[UNIT] --time=DD-HH:MM:SS --pty /bin/bash\n\n\n\n\nsbatch submits a batch script to Slurm. The batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. The batch script may contain options preceded with ‚Äú#SBATCH‚Äù before any executable commands in the script. sbatch will stop processing further #SBATCH directives once the first non-comment non-whitespace line has been reached in the script.\n\n\n\nmybatch.job\n\n#!/usr/bin/env bash\n#\n# This is a script that the SLURM scheduler will use to run your job.\n#\n# The line at the top of the file is called a shebang.\n# It tells the shell what program to use to interpret the script.\n# In this case, we're using bash, the most common shell on Linux systems.\n# The shell is a program that reads commands from the terminal and executes them.\n# It's similar to the command prompt on Windows systems.\n#\n# Lines that begin with # are comments. They are not executed by the shell.\n# They are intended to document what the script does.\n#\n# Lines that begin with #SBATCH are special directives to the SLURM scheduler.\n# These directives tell the scheduler how to run your job.\n# They must come before any executable commands in your script.\n# Remove a single # character from the start of the line to enable a directive.\n# \n# For more information about SLURM directives, see the SLURM documentation:\n# https://slurm.schedmd.com/sbatch.html\n#\n# For more information about bash scripting in general, see:\n# https://www.gnu.org/software/bash/manual/bash.html\n#\n#\n# # JOB OPTIONS\n# You can specify some options here to customize your job.\n# For example, you can give your job a name with the --job-name option,\n# or specify the maximum amount of time the job can run with the --time option.\n#\n#\n# ## JOB NAME\n# The job name will appear when querying running jobs on the system and in log files.\n# The default is the name of this file.\n##SBATCH --job-name=myjob\n#\n#\n# ## RESOURCES\n# SLURM jobs run on compute nodes that are allocated to your job.\n# The resources available to your job depend on the account you are using,\n# the partition you request, and the number of nodes and tasks you request.\n# To see the accounts and partitions available to you, run the command `hyakalloc`.\n#\n##SBATCH --account=&lt;lab&gt; # Replace &lt;lab&gt; with your lab's name, e.g., \"escience\".\n##SBATCH --partition=&lt;partition&gt; # Replace &lt;partition&gt; with the name of the partition you want to use, e.g., \"gpu-a40\".\n##SBATCH --nodes=&lt;number&gt; # Number of nodes to allocate\n##SBATCH --ntasks-per-node=&lt;number&gt; # Number of cores to allocate on each node\n##SBATCH --mem=&lt;number&gt;[units] # How much memory to allocate per node, with units (M|G|T), e.g., 10G = 10 gigabytes\n##SBATCH --gpus=&lt;number&gt; # Number of GPUs to allocate, e.g., 1\n##SBATCH --time=&lt;time&gt; # Max runtime in DD-HH:MM:SS format, e.g., 02-12:00:00 = 1 day, 12 hours\n#\n#\n# ## EMAIL NOTIFICATIONS\n# SLURM can send email notifications when certain events relating to your job occur.\n# To enable email notifications, uncomment the following lines and replace &lt;status&gt; and &lt;email&gt; with appropriate values:\n#\n##SBATCH --mail-type=ALL # Will e-mail about all job state changes\n##SBATCH --mail-user=\"${USER}@uw.edu\" # Who to send e-mail to (here, your username at uw.edu)\n#\n# Some valid --mail-type values besides ALL include:\n#   BEGIN (job started), END (job finished), FAIL (job failed), REQUEUE (job requeued),\n#   STAGE_OUT (stage out completed), TIME_LIMIT (reached max run time), TIME_LIMIT_50 (reached 50% of max run time),\n#   TIME_LIMIT_80 (reached 80% of max run time), TIME_LIMIT_90 (reached 90% of max run time)\n# See sbatch documentation (command: `man sbatch`) for more details.\n#\n# ## WORKING DIRECTORY\n# The --chdir option tells SLURM to change to the specified directory before running your job.\n# The default is to run your job from the current working directory.\n##SBATCH --chdir=&lt;directory&gt;\n#\n#\n# ## JOB OUTPUT STREAMS\n# Processes on Unix/Linux systems write messages to two places:\n# - standard output (stdout): where the program writes its normal output\n# - standard error (stderr): where the program usually writes error and diagnostic messages\n#\n# The default behavior of SLURM is to merge these two streams into a single file,\n# which is written to the current working directory under the name 'slurm-&lt;jobid&gt;.out'.\n# This can be changed with the --output and --error options.\n#\n# To disable an output stream, use /dev/null as the file name -- e.g. --error=/dev/null\n# /dev/null is a special file that discards all data written to it.\n#\n# Uncomment the following lines to customize output and error streams:\n##SBATCH --output=output.log # Where to direct standard output (in this case, to the file 'output.log')\n##SBATCH --error=error.log # Where to direct standard error (in this case, to the file 'error.log')\n#\n#\n# Your program goes here:\necho \"Starting program\"\n&lt;myprogram&gt;\n\nsbatch mybatch.job",
    "crumbs": [
      "Compute",
      "SLURM Basics"
    ]
  },
  {
    "objectID": "docs/compute/slurm.html#checking-access-to-compute-resources",
    "href": "docs/compute/slurm.html#checking-access-to-compute-resources",
    "title": "SLURM Basics",
    "section": "",
    "text": "Use groups command to see which groups you are a member of.\nUse hyakalloc command to check availability of compute resources.",
    "crumbs": [
      "Compute",
      "SLURM Basics"
    ]
  },
  {
    "objectID": "docs/compute/slurm.html#submitting-an-interactive-job-with-salloc",
    "href": "docs/compute/slurm.html#submitting-an-interactive-job-with-salloc",
    "title": "SLURM Basics",
    "section": "",
    "text": "salloc -A &lt;lab&gt; -p &lt;node_type&gt; -N &lt;NUM_NODES&gt; -c &lt;NUM_CPUS&gt; --mem=&lt;MEM&gt;[UNIT] --time=DD-HH:MM:SS\nAdd --no-shell flag if you plan to enter/exit the node without losing the job allocation.\nUse scancel &lt;job_id&gt; to terminate the job allocation.\nUse squeue --me to check information about active or pending job allocations (including job ID).",
    "crumbs": [
      "Compute",
      "SLURM Basics"
    ]
  },
  {
    "objectID": "docs/compute/slurm.html#submitting-an-interactive-job-with-srun",
    "href": "docs/compute/slurm.html#submitting-an-interactive-job-with-srun",
    "title": "SLURM Basics",
    "section": "",
    "text": "From the login node, run the following:\nsrun -A &lt;lab&gt; -p &lt;node_type&gt; -N &lt;NUM_NODES&gt; -c &lt;NUM_CPUS&gt; --mem=&lt;MEM&gt;[UNIT] --time=DD-HH:MM:SS --pty /bin/bash",
    "crumbs": [
      "Compute",
      "SLURM Basics"
    ]
  },
  {
    "objectID": "docs/compute/slurm.html#submitting-a-batch-job",
    "href": "docs/compute/slurm.html#submitting-a-batch-job",
    "title": "SLURM Basics",
    "section": "",
    "text": "sbatch submits a batch script to Slurm. The batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. The batch script may contain options preceded with ‚Äú#SBATCH‚Äù before any executable commands in the script. sbatch will stop processing further #SBATCH directives once the first non-comment non-whitespace line has been reached in the script.\n\n\n\nmybatch.job\n\n#!/usr/bin/env bash\n#\n# This is a script that the SLURM scheduler will use to run your job.\n#\n# The line at the top of the file is called a shebang.\n# It tells the shell what program to use to interpret the script.\n# In this case, we're using bash, the most common shell on Linux systems.\n# The shell is a program that reads commands from the terminal and executes them.\n# It's similar to the command prompt on Windows systems.\n#\n# Lines that begin with # are comments. They are not executed by the shell.\n# They are intended to document what the script does.\n#\n# Lines that begin with #SBATCH are special directives to the SLURM scheduler.\n# These directives tell the scheduler how to run your job.\n# They must come before any executable commands in your script.\n# Remove a single # character from the start of the line to enable a directive.\n# \n# For more information about SLURM directives, see the SLURM documentation:\n# https://slurm.schedmd.com/sbatch.html\n#\n# For more information about bash scripting in general, see:\n# https://www.gnu.org/software/bash/manual/bash.html\n#\n#\n# # JOB OPTIONS\n# You can specify some options here to customize your job.\n# For example, you can give your job a name with the --job-name option,\n# or specify the maximum amount of time the job can run with the --time option.\n#\n#\n# ## JOB NAME\n# The job name will appear when querying running jobs on the system and in log files.\n# The default is the name of this file.\n##SBATCH --job-name=myjob\n#\n#\n# ## RESOURCES\n# SLURM jobs run on compute nodes that are allocated to your job.\n# The resources available to your job depend on the account you are using,\n# the partition you request, and the number of nodes and tasks you request.\n# To see the accounts and partitions available to you, run the command `hyakalloc`.\n#\n##SBATCH --account=&lt;lab&gt; # Replace &lt;lab&gt; with your lab's name, e.g., \"escience\".\n##SBATCH --partition=&lt;partition&gt; # Replace &lt;partition&gt; with the name of the partition you want to use, e.g., \"gpu-a40\".\n##SBATCH --nodes=&lt;number&gt; # Number of nodes to allocate\n##SBATCH --ntasks-per-node=&lt;number&gt; # Number of cores to allocate on each node\n##SBATCH --mem=&lt;number&gt;[units] # How much memory to allocate per node, with units (M|G|T), e.g., 10G = 10 gigabytes\n##SBATCH --gpus=&lt;number&gt; # Number of GPUs to allocate, e.g., 1\n##SBATCH --time=&lt;time&gt; # Max runtime in DD-HH:MM:SS format, e.g., 02-12:00:00 = 1 day, 12 hours\n#\n#\n# ## EMAIL NOTIFICATIONS\n# SLURM can send email notifications when certain events relating to your job occur.\n# To enable email notifications, uncomment the following lines and replace &lt;status&gt; and &lt;email&gt; with appropriate values:\n#\n##SBATCH --mail-type=ALL # Will e-mail about all job state changes\n##SBATCH --mail-user=\"${USER}@uw.edu\" # Who to send e-mail to (here, your username at uw.edu)\n#\n# Some valid --mail-type values besides ALL include:\n#   BEGIN (job started), END (job finished), FAIL (job failed), REQUEUE (job requeued),\n#   STAGE_OUT (stage out completed), TIME_LIMIT (reached max run time), TIME_LIMIT_50 (reached 50% of max run time),\n#   TIME_LIMIT_80 (reached 80% of max run time), TIME_LIMIT_90 (reached 90% of max run time)\n# See sbatch documentation (command: `man sbatch`) for more details.\n#\n# ## WORKING DIRECTORY\n# The --chdir option tells SLURM to change to the specified directory before running your job.\n# The default is to run your job from the current working directory.\n##SBATCH --chdir=&lt;directory&gt;\n#\n#\n# ## JOB OUTPUT STREAMS\n# Processes on Unix/Linux systems write messages to two places:\n# - standard output (stdout): where the program writes its normal output\n# - standard error (stderr): where the program usually writes error and diagnostic messages\n#\n# The default behavior of SLURM is to merge these two streams into a single file,\n# which is written to the current working directory under the name 'slurm-&lt;jobid&gt;.out'.\n# This can be changed with the --output and --error options.\n#\n# To disable an output stream, use /dev/null as the file name -- e.g. --error=/dev/null\n# /dev/null is a special file that discards all data written to it.\n#\n# Uncomment the following lines to customize output and error streams:\n##SBATCH --output=output.log # Where to direct standard output (in this case, to the file 'output.log')\n##SBATCH --error=error.log # Where to direct standard error (in this case, to the file 'error.log')\n#\n#\n# Your program goes here:\necho \"Starting program\"\n&lt;myprogram&gt;\n\nsbatch mybatch.job",
    "crumbs": [
      "Compute",
      "SLURM Basics"
    ]
  },
  {
    "objectID": "docs/start/klone-storage.html",
    "href": "docs/start/klone-storage.html",
    "title": "Storage on Klone",
    "section": "",
    "text": "Klone has a number of storage options available to users. This page describes the options and best practices for using them.\n\n\nThe Klone cluster makes available several locations for storing data, some of which are shared across the login nodes and compute nodes, and some of which are only available to the node where a job is running.\n\n\nThe following storage locations are shared across the login nodes and compute nodes:\n\n\nEvery user on Klone has a home directory, which is the default location for storing files. The home directory is located at /mmfs1/home/&lt;username&gt;. The home directory is where you are when you log in to the login node, and it is also acessible by any compute node where you have a job running.\n\nNone of the storage locations on Klone are backed up. If you need to store data that you cannot afford to lose, you should use a different storage location ‚Äì see our guide to storage for more information.\n\nCurrently, there is a 10 GiB quota on the home directory. This quota is enforced by the system, and you will not be able to write to your home directory if you exceed it. You can check your quota with the hyakstorage command when you are logged in to any Klone node:\n1hyakstorage --home\n\n1\n\nThe --home flag tells hyakstorage to check your home directory quota.\n\n\nThe result will look something like this:\n                       Usage report for /mmfs1/home/altan                       \n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                   ‚îÇ Disk Usage                 ‚îÇ Files Usage                 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Total:            ‚îÇ 1GB / 10GB                 ‚îÇ 11579 / 256000 files        ‚îÇ\n‚îÇ                   ‚îÇ 10%                        ‚îÇ 5%                          ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\nIt is important to note that the home directory is not intended for storing large amounts of data. If you need to store more than a few GiB of data, you should use one of the other storage options described below.\n\nA lot of software will save files to your home directory by default. Python, for example, will install packages to ~/.local/lib/python3.6 by default when they are installed using pip install --user. Apptainer will also cache images in ~/.apptainer/cache by default. Fortunately, both of these can be changed by setting environment variables or using solutions like virtual environments.\nIf you are installing software, you should always check to see where it is being installed, and make sure that it is not being installed to your home directory unless you have a good reason for doing so.\n\n\n\n\nEvery group on Klone has a directory under /mmfs1/gscratch (also aliased as /gscratch), which is a good place to store data that you are using for a job, as well as any data and libraries you install. The gscratch directory is accessible from any node.\nEach group has a quota on their gscratch directory, and you will not be able to write to your group‚Äôs gscratch directory if you exceed it.\nYou can find out the locataion of your group‚Äôs gscratch directory and see how much is used of your available capacity using the hyakstorage command when you are logged in to any Klone node:\n1hyakstorage --gscratch\n\n1\n\nThe --gscratch flag tells hyakstorage to check your group‚Äôs gscratch quota.\n\n\nThe result will look something like this:\n                   Usage report for /mmfs1/gscratch/escience                    \n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                   ‚îÇ Disk Usage                 ‚îÇ Files Usage                 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Total:            ‚îÇ 3305GB / 4096GB            ‚îÇ 710698 / 4000000 files      ‚îÇ\n‚îÇ                   ‚îÇ 81%                        ‚îÇ 18%                         ‚îÇ\n‚îÇ My usage:         ‚îÇ 41GB                       ‚îÇ 11776 files                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\nThe line at the top shows the location of your group‚Äôs gscratch directory.\n\nEvery lab or group has its own quota for /gscratch storage. By default, the quota is 1 TiB per slice (or 4 TiB for a GPU slice). Additional storage is available at $10/TiB/month as of 2023-12-08.\n\n\n\n\nThere is also a directory /gscratch/scrubbed, which is a scratch directory where any data not accessed in 21 days is automatically deleted. This directory is slower than other directories under /gscratch. There is no quota on this directory, but you should not store data here that you cannot afford to lose.\n\n\n\n\n\n\nhttps://hyak.uw.edu/blog/klone-storage-update\nhttps://hyak.uw.edu/docs/storage",
    "crumbs": [
      "Getting Started",
      "Storage on Klone"
    ]
  },
  {
    "objectID": "docs/start/klone-storage.html#overview",
    "href": "docs/start/klone-storage.html#overview",
    "title": "Storage on Klone",
    "section": "",
    "text": "The Klone cluster makes available several locations for storing data, some of which are shared across the login nodes and compute nodes, and some of which are only available to the node where a job is running.\n\n\nThe following storage locations are shared across the login nodes and compute nodes:\n\n\nEvery user on Klone has a home directory, which is the default location for storing files. The home directory is located at /mmfs1/home/&lt;username&gt;. The home directory is where you are when you log in to the login node, and it is also acessible by any compute node where you have a job running.\n\nNone of the storage locations on Klone are backed up. If you need to store data that you cannot afford to lose, you should use a different storage location ‚Äì see our guide to storage for more information.\n\nCurrently, there is a 10 GiB quota on the home directory. This quota is enforced by the system, and you will not be able to write to your home directory if you exceed it. You can check your quota with the hyakstorage command when you are logged in to any Klone node:\n1hyakstorage --home\n\n1\n\nThe --home flag tells hyakstorage to check your home directory quota.\n\n\nThe result will look something like this:\n                       Usage report for /mmfs1/home/altan                       \n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                   ‚îÇ Disk Usage                 ‚îÇ Files Usage                 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Total:            ‚îÇ 1GB / 10GB                 ‚îÇ 11579 / 256000 files        ‚îÇ\n‚îÇ                   ‚îÇ 10%                        ‚îÇ 5%                          ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\nIt is important to note that the home directory is not intended for storing large amounts of data. If you need to store more than a few GiB of data, you should use one of the other storage options described below.\n\nA lot of software will save files to your home directory by default. Python, for example, will install packages to ~/.local/lib/python3.6 by default when they are installed using pip install --user. Apptainer will also cache images in ~/.apptainer/cache by default. Fortunately, both of these can be changed by setting environment variables or using solutions like virtual environments.\nIf you are installing software, you should always check to see where it is being installed, and make sure that it is not being installed to your home directory unless you have a good reason for doing so.\n\n\n\n\nEvery group on Klone has a directory under /mmfs1/gscratch (also aliased as /gscratch), which is a good place to store data that you are using for a job, as well as any data and libraries you install. The gscratch directory is accessible from any node.\nEach group has a quota on their gscratch directory, and you will not be able to write to your group‚Äôs gscratch directory if you exceed it.\nYou can find out the locataion of your group‚Äôs gscratch directory and see how much is used of your available capacity using the hyakstorage command when you are logged in to any Klone node:\n1hyakstorage --gscratch\n\n1\n\nThe --gscratch flag tells hyakstorage to check your group‚Äôs gscratch quota.\n\n\nThe result will look something like this:\n                   Usage report for /mmfs1/gscratch/escience                    \n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ                   ‚îÇ Disk Usage                 ‚îÇ Files Usage                 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Total:            ‚îÇ 3305GB / 4096GB            ‚îÇ 710698 / 4000000 files      ‚îÇ\n‚îÇ                   ‚îÇ 81%                        ‚îÇ 18%                         ‚îÇ\n‚îÇ My usage:         ‚îÇ 41GB                       ‚îÇ 11776 files                 ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\nThe line at the top shows the location of your group‚Äôs gscratch directory.\n\nEvery lab or group has its own quota for /gscratch storage. By default, the quota is 1 TiB per slice (or 4 TiB for a GPU slice). Additional storage is available at $10/TiB/month as of 2023-12-08.\n\n\n\n\nThere is also a directory /gscratch/scrubbed, which is a scratch directory where any data not accessed in 21 days is automatically deleted. This directory is slower than other directories under /gscratch. There is no quota on this directory, but you should not store data here that you cannot afford to lose.",
    "crumbs": [
      "Getting Started",
      "Storage on Klone"
    ]
  },
  {
    "objectID": "docs/start/klone-storage.html#references",
    "href": "docs/start/klone-storage.html#references",
    "title": "Storage on Klone",
    "section": "",
    "text": "https://hyak.uw.edu/blog/klone-storage-update\nhttps://hyak.uw.edu/docs/storage",
    "crumbs": [
      "Getting Started",
      "Storage on Klone"
    ]
  },
  {
    "objectID": "docs/start/connect-ssh.html",
    "href": "docs/start/connect-ssh.html",
    "title": "Connecting to HYAK with SSH",
    "section": "",
    "text": "SSH (secure shell) is the primary method for connecting to and interacting with UW HYAK clusters from a command-line interface (CLI).\n\n\n\n\n\n\nImportant\n\n\n\nConnections to UW HYAK are authenticated with your UW NetID credentials and Duo for two-factor. Alternative login methods, including SSH key authentication, cannot be used to login.\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse an SSH client that supports session multiplexing/sharing to reuse an active session without two-factor authentication.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nmacOS, most Linux distributions, and (newer builds of) Windows 10/11 already include OpenSSH client by default.\n\n\nPick your platform and install an SSH client:\n\nLinuxmacOSWindows\n\n\nOpen a terminal and install OpenSSH client with your distribution‚Äôs package manager.\n\n\n\nDistribution\nCommand\n\n\n\n\nUbuntu/Debian/Linux Mint\nsudo apt-get install openssh\n\n\nRHEL/CentOS/Fedora/Rocky\nsudo dnf install openssh\n\n\nSUSE\nsudo zypper install openssh\n\n\nAlpine\napk add openssh\n\n\nArch\nsudo pacman -S openssh\n\n\n\nTo setup session sharing, create a new host entry to your local computer‚Äôs ~/.ssh/config with a text editor:\n\n\n~/.ssh/config\n\nHost klone.hyak.uw.edu\n    HostName %h\n    ControlPath ~/.ssh/%r@%h:%p\n    ControlMaster auto\n    ControlPersist 3600\n\n\n\nmacOS provides SSH commands by default and are accessible from command-line with the native macOS Terminal application.\nTo setup session sharing, create a new host entry to your local computer‚Äôs ~/.ssh/config with a text editor:\n\n\n~/.ssh/config\n\nHost klone.hyak.uw.edu\n    HostName %h\n    ControlPath ~/.ssh/%r@%h:%p\n    ControlMaster auto\n    ControlPersist 3600\n\n\n\nThere are many SSH clients available for the Windows platform. Here is a short table comparing features provided by each SSH client:\n:::{table} Feature Comparison of Windows SSH Clients | SSH Client | Port-Forwarding | X11 | Session Sharing | Interface | File Transfer Interface | | :‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî‚Äî‚Äî | ‚Äî | ‚Äî‚Äî‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì | | Win32-OpenSSH | Supported | Requires X11 Server[^x11] | Unsupported | CLI | CLI | | MobaXterm | Supported | Supported | Unsupported | GUI/CLI | GUI/CLI | | PuTTY | Supported | Requires X11 Server[^x11] | Supported | GUI | CLI | | MSYS2+OpenSSH | Supported | Requires X11 Server[^x11] | Unsupported | CLI | CLI | | WSL2+OpenSSH | Supported | Supported | Supported | CLI | CLI |\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPuTTY‚Äôs plink.exe tool is not suited for interactive sessions as it cannot handle many key inputs (arrow keys for cursor movement, backspace for character deletion).\n\n\n\n\n\n\n\n\nSession Sharing in PuTTY\n\n\n\n\n\nCheck Share SSH Connection if possible in the PuTTY Configuration window under Connection-&gt;SSH:\n\nSave a PuTTY profile under Session with the hostname set to UWNetID@klone.hyak.uw.edu:\n\nWhile PuTTY maintains an active session with session sharing enabled, all (GUI/CLI) PuTTY tools can reuse the active authenticated session as long as the terminal window remains open and active. If all sessions close, authentication will be required.\nTo create a new terminal window, right click the title bar of an active terminal window, then click on Duplicate Session.\nTo reuse an active session with CLI tools, specify the name of the saved PuTTY profile in place of UWNetID@klone.hyak.uw.edu.\n\n\n\n\n\n\n\nOpenSSHPuTTYMobaXterm GUI\n\n\n\nOpen a terminal instance.\n\n\n\n\n\n\n\nNote\n\n\n\nWindows users should open PowerShell console, or install and use Windows Terminal from the Microsoft Store app.\n\n\n\nConnect to HYAK Klone cluster with ssh command with your UW NetID:\n\n\nssh UWNetID@klone.hyak.uw.edu  \n\nIf prompted to ‚Äúcontinue connecting‚Äù, type yes and press enter.\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.\n\n\n\nOpen PuTTY:\n\n\n\nCheck Share SSH Connection if possible under Connection-&gt;SSH:\n\n\n\nUnder Session, set the hostname to UWNetID@klone.hyak.uw.edu and save the profile as Klone:\n\n\n\nPress Open at the bottom of the configuration window to start the connection.\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.\n\n\n\nOpen MobaXterm:\n\n\n\nClick the Session icon in the top left corner.\nSelect SSH under the Session settings window, then do the following:\n\n\nset the remote host to klone.hyak.uw.edu\ncheck Specify username and specify your UW NetID\n\n\n\nPress OK at the bottom of the window to start the connection.\nPress Accept if prompted to trust the identity of the remote host:\n\n\n\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.\n\n\n\n\n\n\n\nOpenSSHPuTTY CLI\n\n\nscp is an OpenSSH client utility for copying files and directories to/from a remote target. The general syntax has the following form:\nscp [-r] &lt;SOURCE_PATH&gt; UWNetID@klone.hyak.uw.edu:&lt;DESTINATION_PATH&gt;\nscp [-r] UWNetID@klone.hyak.uw.edu:&lt;SOURCE_PATH&gt; &lt;DESTINATION_PATH&gt;\n\n\n\n\n\n\nTip\n\n\n\nTo copy a file to Klone at some path, run:\nscp /path/to/my/file UWNetID@klone.hyak.uw.edu:/gscratch/mylab/\n\n\n\n\npscp is a CLI utility (provided by a standard PuTTY installation) for copying files.\npscp [-r] &lt;SOURCE_PATH&gt; UWNetID@klone.hyak.uw.edu:&lt;DESTINATION_PATH&gt;\npscp [-r] UWNetID@klone.hyak.uw.edu:&lt;SOURCE_PATH&gt; &lt;DESTINATION_PATH&gt;\n\n\n\n\n\n\nNote\n\n\n\npscp does not support the use of the tilde (~) as a shortcut to the home directory. By default, relative paths always start from home directory anyways.\n# copy file to home directory\npscp.exe \"c:\\path\\to\\my\\file\" UWNetID@klone.hyak.uw.edu:\n\n\n\n\n\n\n\n\nTip\n\n\n\npscp can reuse/share an active SSH connection without re-authorization if using a saved PuTTY profile with Share SSH Connection if possible enabled.\n\n\nTo send a file to Klone at some path, run:\npscp.exe \"c:\\path\\to\\my\\file\" UWNetID@klone.hyak.uw.edu:/gscratch/mylab/\nAlternatively, we can use the name of the saved PuTTY profile (Klone for this example) to reuse an active connection to copy a file to Klone:\npscp.exe \"c:\\path\\to\\my\\file\" Klone:/gscratch/mylab/\nTo copy a directory to Klone, use the -r argument to copy directories and files recursively:\npscp.exe -r c:\\path\\to\\my\\directory\\ Klone:/gscratch/mylab/",
    "crumbs": [
      "Getting Started",
      "Connecting to HYAK with SSH"
    ]
  },
  {
    "objectID": "docs/start/connect-ssh.html#installing-an-ssh-client",
    "href": "docs/start/connect-ssh.html#installing-an-ssh-client",
    "title": "Connecting to HYAK with SSH",
    "section": "",
    "text": "Note\n\n\n\nmacOS, most Linux distributions, and (newer builds of) Windows 10/11 already include OpenSSH client by default.\n\n\nPick your platform and install an SSH client:\n\nLinuxmacOSWindows\n\n\nOpen a terminal and install OpenSSH client with your distribution‚Äôs package manager.\n\n\n\nDistribution\nCommand\n\n\n\n\nUbuntu/Debian/Linux Mint\nsudo apt-get install openssh\n\n\nRHEL/CentOS/Fedora/Rocky\nsudo dnf install openssh\n\n\nSUSE\nsudo zypper install openssh\n\n\nAlpine\napk add openssh\n\n\nArch\nsudo pacman -S openssh\n\n\n\nTo setup session sharing, create a new host entry to your local computer‚Äôs ~/.ssh/config with a text editor:\n\n\n~/.ssh/config\n\nHost klone.hyak.uw.edu\n    HostName %h\n    ControlPath ~/.ssh/%r@%h:%p\n    ControlMaster auto\n    ControlPersist 3600\n\n\n\nmacOS provides SSH commands by default and are accessible from command-line with the native macOS Terminal application.\nTo setup session sharing, create a new host entry to your local computer‚Äôs ~/.ssh/config with a text editor:\n\n\n~/.ssh/config\n\nHost klone.hyak.uw.edu\n    HostName %h\n    ControlPath ~/.ssh/%r@%h:%p\n    ControlMaster auto\n    ControlPersist 3600\n\n\n\nThere are many SSH clients available for the Windows platform. Here is a short table comparing features provided by each SSH client:\n:::{table} Feature Comparison of Windows SSH Clients | SSH Client | Port-Forwarding | X11 | Session Sharing | Interface | File Transfer Interface | | :‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî‚Äî‚Äî | ‚Äî | ‚Äî‚Äî‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì | | Win32-OpenSSH | Supported | Requires X11 Server[^x11] | Unsupported | CLI | CLI | | MobaXterm | Supported | Supported | Unsupported | GUI/CLI | GUI/CLI | | PuTTY | Supported | Requires X11 Server[^x11] | Supported | GUI | CLI | | MSYS2+OpenSSH | Supported | Requires X11 Server[^x11] | Unsupported | CLI | CLI | | WSL2+OpenSSH | Supported | Supported | Supported | CLI | CLI |\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPuTTY‚Äôs plink.exe tool is not suited for interactive sessions as it cannot handle many key inputs (arrow keys for cursor movement, backspace for character deletion).\n\n\n\n\n\n\n\n\nSession Sharing in PuTTY\n\n\n\n\n\nCheck Share SSH Connection if possible in the PuTTY Configuration window under Connection-&gt;SSH:\n\nSave a PuTTY profile under Session with the hostname set to UWNetID@klone.hyak.uw.edu:\n\nWhile PuTTY maintains an active session with session sharing enabled, all (GUI/CLI) PuTTY tools can reuse the active authenticated session as long as the terminal window remains open and active. If all sessions close, authentication will be required.\nTo create a new terminal window, right click the title bar of an active terminal window, then click on Duplicate Session.\nTo reuse an active session with CLI tools, specify the name of the saved PuTTY profile in place of UWNetID@klone.hyak.uw.edu.",
    "crumbs": [
      "Getting Started",
      "Connecting to HYAK with SSH"
    ]
  },
  {
    "objectID": "docs/start/connect-ssh.html#connecting-via-ssh",
    "href": "docs/start/connect-ssh.html#connecting-via-ssh",
    "title": "Connecting to HYAK with SSH",
    "section": "",
    "text": "OpenSSHPuTTYMobaXterm GUI\n\n\n\nOpen a terminal instance.\n\n\n\n\n\n\n\nNote\n\n\n\nWindows users should open PowerShell console, or install and use Windows Terminal from the Microsoft Store app.\n\n\n\nConnect to HYAK Klone cluster with ssh command with your UW NetID:\n\n\nssh UWNetID@klone.hyak.uw.edu  \n\nIf prompted to ‚Äúcontinue connecting‚Äù, type yes and press enter.\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.\n\n\n\nOpen PuTTY:\n\n\n\nCheck Share SSH Connection if possible under Connection-&gt;SSH:\n\n\n\nUnder Session, set the hostname to UWNetID@klone.hyak.uw.edu and save the profile as Klone:\n\n\n\nPress Open at the bottom of the configuration window to start the connection.\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.\n\n\n\nOpen MobaXterm:\n\n\n\nClick the Session icon in the top left corner.\nSelect SSH under the Session settings window, then do the following:\n\n\nset the remote host to klone.hyak.uw.edu\ncheck Specify username and specify your UW NetID\n\n\n\nPress OK at the bottom of the window to start the connection.\nPress Accept if prompted to trust the identity of the remote host:\n\n\n\nLogin with your NetID password and authenticate with Duo.\n\nIf successful, the login node‚Äôs command prompt should appear.",
    "crumbs": [
      "Getting Started",
      "Connecting to HYAK with SSH"
    ]
  },
  {
    "objectID": "docs/start/connect-ssh.html#transferring-files-tofrom-hyak",
    "href": "docs/start/connect-ssh.html#transferring-files-tofrom-hyak",
    "title": "Connecting to HYAK with SSH",
    "section": "",
    "text": "OpenSSHPuTTY CLI\n\n\nscp is an OpenSSH client utility for copying files and directories to/from a remote target. The general syntax has the following form:\nscp [-r] &lt;SOURCE_PATH&gt; UWNetID@klone.hyak.uw.edu:&lt;DESTINATION_PATH&gt;\nscp [-r] UWNetID@klone.hyak.uw.edu:&lt;SOURCE_PATH&gt; &lt;DESTINATION_PATH&gt;\n\n\n\n\n\n\nTip\n\n\n\nTo copy a file to Klone at some path, run:\nscp /path/to/my/file UWNetID@klone.hyak.uw.edu:/gscratch/mylab/\n\n\n\n\npscp is a CLI utility (provided by a standard PuTTY installation) for copying files.\npscp [-r] &lt;SOURCE_PATH&gt; UWNetID@klone.hyak.uw.edu:&lt;DESTINATION_PATH&gt;\npscp [-r] UWNetID@klone.hyak.uw.edu:&lt;SOURCE_PATH&gt; &lt;DESTINATION_PATH&gt;\n\n\n\n\n\n\nNote\n\n\n\npscp does not support the use of the tilde (~) as a shortcut to the home directory. By default, relative paths always start from home directory anyways.\n# copy file to home directory\npscp.exe \"c:\\path\\to\\my\\file\" UWNetID@klone.hyak.uw.edu:\n\n\n\n\n\n\n\n\nTip\n\n\n\npscp can reuse/share an active SSH connection without re-authorization if using a saved PuTTY profile with Share SSH Connection if possible enabled.\n\n\nTo send a file to Klone at some path, run:\npscp.exe \"c:\\path\\to\\my\\file\" UWNetID@klone.hyak.uw.edu:/gscratch/mylab/\nAlternatively, we can use the name of the saved PuTTY profile (Klone for this example) to reuse an active connection to copy a file to Klone:\npscp.exe \"c:\\path\\to\\my\\file\" Klone:/gscratch/mylab/\nTo copy a directory to Klone, use the -r argument to copy directories and files recursively:\npscp.exe -r c:\\path\\to\\my\\directory\\ Klone:/gscratch/mylab/",
    "crumbs": [
      "Getting Started",
      "Connecting to HYAK with SSH"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html",
    "href": "docs/start/hyakvnc.html",
    "title": "hyakvnc",
    "section": "",
    "text": "hyakvnc is a program to create and manage VNC sessions running within an Apptainer environment on a compute node.\n\n\n\n\n\n\nNote\n\n\n\nhyakvnc script is ran from the login node and will interact with Slurm to spawn a compute node for interactive use.\n\n\n\n\nOn the user‚Äôs computer, we will need the following: - SSH client with port-forwarding capabilities - VNC client or viewer to interact with the remote graphical interface - for all platforms, we recommend TigerVNC viewer: - macOS: https://sourceforge.net/projects/tigervnc/files/stable/1.13.1/TigerVNC-1.13.1.dmg - Windows (x86_64): https://sourceforge.net/projects/tigervnc/files/stable/1.13.1/vncviewer64-1.13.1.exe\n\n\n\n\n\n\nNote\n\n\n\nTigerVNC viewer supports clipboard sharing and dynamically adjusts resolution upon changing window size.\n\n\n\n\n\nA VNC Apptainer hosts the graphical desktop interface, CLI/GUI tools, and libraries needed to run programs.\nAt a bare minimum for hyakvnc, this container must have the following: - TigerVNC server (with vncserver and vncpasswd) - a desktop environment (such as XFCE4)\n\n\n\n\n\n\nTip\n\n\n\nFor a more functional environment, we recommend installing additional tools and libraries as if it were your own computer.\n\n\n\n\n\n\n\n\nNote\n\n\n\nA container can be maintained by individual users or entire groups.\n\n\n\n\nHere, we will build a VNC Apptainer from https://github.com/uw-psych/hyak_vnc_apptainer. Feel free to make adjustments to these recipes as needed.\n\nFrom the login node, get an interactive node for 4 hours:\n\n[NetID@klone-login01 ~]$ salloc -A &lt;lab&gt; -p &lt;node_type&gt; -c 4 --mem=8G --time=4:00:00\n\nClone the repository to a directory with around 4GB of free space:\n\ncd some_place_with_space\ngit clone https://github.com/uw-psych/hyak_vnc_apptainer\ncd hyak_vnc_apptainer\n\nLoad Apptainer module:\n\nmodule load apptainer/1.1.5\n\nBuild rockylinux8 container by running the following:\n\nmake CONT_NAME=rockylinux8\n\nGet the path to the container and xstartup:\n\nrealpath rockylinux8/rockylinux8.sif\nrealpath xfce4_config/xstartup\n\n\n\n\nFrom the login node, run the following to install or upgrade hyakvnc:\npython3.6 -m pip install --upgrade pip\npython3.6 -m pip install git+https://github.com/uw-psych/hyakvnc\nOnce installed, run hyakvnc -h to print usage help:\n$ hyakvnc -h\nusage: hyakvnc [-h] [-d] [-v] [-J &lt;job_name&gt;] {create,status,kill,kill-all,set-passwd,repair} ...\n\npositional arguments:\n  {create,status,kill,kill-all,set-passwd,repair}\n    create              Create VNC session\n    status              Print details of all VNC jobs with given job name and exit\n    kill                Kill specified job\n    kill-all            Cancel all VNC jobs with given job name and exit\n    set-passwd          Prompts for new VNC password and exit\n    repair              Repair all missing/broken LoginNode&lt;-&gt;SubNode port forwards, and then exit\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d, --debug           Enable debug logging\n  -v, --version         Print program version and exit\n  -J &lt;job_name&gt;         Slurm job name\nCheck usage help for a specific hyakvnc command by running hyakvnc &lt;command&gt; -h.\n\n\n\n\nVNC session cannot be reserved indefinitely and will terminate when its time expires.\n\n\n\n\n\n\n\nNote\n\n\n\nIf hyakvnc create fails to reserve a node and timesout, then check if you are reducing too many compute resources.\n\n\n\nFrom the login node shell, we will need to use hyakvnc create command to start the VNC session with compute resources. As an example, to create a VNC session running on our rockylinux8 container on a compute node reserved for 10 hours with specified amount of compute resources, run the following:\n\nhyakvnc create \\\n    -A &lt;lab&gt; \\\n    -p &lt;node_type&gt; \\\n    --cpus 8 \\\n    --gpus 0 \\\n    --mem 32G \\\n    --time 10 \\\n    --container /path/to/hyak_vnc_apptainer/rockylinux8/rockylinux8.sif \\\n    --xstartup /path/to/hyak_vnc_apptainer/xfce4_config/xstartup\nOn initial use, accept to create an SSH key and set a VNC password when prompted. If you want to reset your VNC password, run hyakvnc set-passwd from the login node.\n\nCreate a port forward to Klone to access the VNC session.\n\nIf hyakvnc create was successful, it should print additional steps to setup a port forward and to connect to the VNC session.\n...\n=====================\nRun the following in a new terminal window:\n        ssh -N -f -L 590A:127.0.0.1:590B UWNetID@klone.hyak.uw.edu\nthen connect to VNC session at localhost:590A\n=====================\nIn this example, we want to create a port forward to Klone with the source port 590A and destination at 127.0.0.1:590B or localhost:590B.\n\nOpenSSH (macOS, Linux, Windows)\n\n\nCreate a new terminal window on your local computer and run the ssh command as instructed by hyakvnc create:\nssh -N -f -L 590A:127.0.0.1:590B UWNetID@klone.hyak.uw.edu\n\n\n\n\n\n\n\nInstead of creating a new terminal window, right-click the title bar of an existing terminal window, then select Change Settings:\n\n\n\nNavigate to Connection-&gt;SSH-&gt;Tunnels. Set Source port to 590A and Destination to 127.0.0.1:590B.\n\n\n\nClick on Add to add the port forward, then click on Apply at the bottom of the window.\n\n\n\n\n\n\n\nNote\n\n\n\nClosing all PuTTY terminal windows will close the port forward and break the VNC connection.\n\n\n::: :::\n\nOpen TigerVNC Viewer and connect to the VNC session at localhost:590A as instructed by hyakvnc create:\n\n\n\nLogin with your VNC password:\n\n\nIf successful, you should be greeted to a graphical desktop interface.\n\n\n\nRunning hyakvnc status from the login node will print details of active VNC sessions (with the same job name), including the following: - Slurm Job ID - Node hostname - Time left in days-hours:minutes:seconds - VNC status - VNC port forward command\nActive hyakvnc jobs:\n        Job ID: AAAAAAAA\n                SubNode: n3301\n                Time left: 2-06:00:16\n                VNC active: True\n                VNC display number: 2\n                VNC port: 5902\n                Mapped LoginNode port: 5900\n                Run command: ssh -N -f -L 5900:127.0.0.1:5900 UWNetID@klone.hyak.uw.edu\n        Job ID: BBBBBBBB\n                SubNode: n3301\n                Time left: 2-01:24:25\n                VNC active: True\n                VNC display number: 1\n                VNC port: 5901\n                Mapped LoginNode port: 5911\n                Run command: ssh -N -f -L 5911:127.0.0.1:5911 UWNetID@klone.hyak.uw.edu\n        Job ID: CCCCCCCC\n                SubNode: g3071\n                Time left: 15:07:56\n                VNC active: True\n                VNC display number: 1\n                VNC port: 5901\n                Mapped LoginNode port: 5902\n                Run command: ssh -N -f -L 5902:127.0.0.1:5902 UWNetID@klone.hyak.uw.edu\n\n\n\nTo close all VNC sessions, run hyakvnc kill-all from the login node.\nTo close a specific VNC session, find its Job ID with hyakvnc status, then run hyakvnc kill &lt;job_ID&gt; from the login node.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRequires the VNC apptainer to support Lmod. /sw/ must also be bounded to access existing module files.\n\n\nIf the VNC apptainer supports Lmod, then module command should work normally without throwing an error.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRequires the VNC apptainer to support Lmod and supply Apptainer dependencies.\n\n\nLoad apptainer module with a version specified (using default will not work unless apptainer is installed in the container):\nmodule load apptainer/1.1.5\nThen apptainer commands will work normally:\nExample:\napptainer shell &lt;container.sif&gt;\napptainer exec &lt;container.sif&gt; &lt;command&gt;\n\n\nTo load a nested Apptainer within the VNC session with CUDA support, add the following arguments: --nv -B /.singularity.d.\nExample:\napptainer shell --nv -B /.singularity.d &lt;container.sif&gt;\n\n\n\n\nhttps://github.com/uw-psych/hyakvnc",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#prerequisites",
    "href": "docs/start/hyakvnc.html#prerequisites",
    "title": "hyakvnc",
    "section": "",
    "text": "On the user‚Äôs computer, we will need the following: - SSH client with port-forwarding capabilities - VNC client or viewer to interact with the remote graphical interface - for all platforms, we recommend TigerVNC viewer: - macOS: https://sourceforge.net/projects/tigervnc/files/stable/1.13.1/TigerVNC-1.13.1.dmg - Windows (x86_64): https://sourceforge.net/projects/tigervnc/files/stable/1.13.1/vncviewer64-1.13.1.exe\n\n\n\n\n\n\nNote\n\n\n\nTigerVNC viewer supports clipboard sharing and dynamically adjusts resolution upon changing window size.",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#vnc-apptainer",
    "href": "docs/start/hyakvnc.html#vnc-apptainer",
    "title": "hyakvnc",
    "section": "",
    "text": "A VNC Apptainer hosts the graphical desktop interface, CLI/GUI tools, and libraries needed to run programs.\nAt a bare minimum for hyakvnc, this container must have the following: - TigerVNC server (with vncserver and vncpasswd) - a desktop environment (such as XFCE4)\n\n\n\n\n\n\nTip\n\n\n\nFor a more functional environment, we recommend installing additional tools and libraries as if it were your own computer.\n\n\n\n\n\n\n\n\nNote\n\n\n\nA container can be maintained by individual users or entire groups.\n\n\n\n\nHere, we will build a VNC Apptainer from https://github.com/uw-psych/hyak_vnc_apptainer. Feel free to make adjustments to these recipes as needed.\n\nFrom the login node, get an interactive node for 4 hours:\n\n[NetID@klone-login01 ~]$ salloc -A &lt;lab&gt; -p &lt;node_type&gt; -c 4 --mem=8G --time=4:00:00\n\nClone the repository to a directory with around 4GB of free space:\n\ncd some_place_with_space\ngit clone https://github.com/uw-psych/hyak_vnc_apptainer\ncd hyak_vnc_apptainer\n\nLoad Apptainer module:\n\nmodule load apptainer/1.1.5\n\nBuild rockylinux8 container by running the following:\n\nmake CONT_NAME=rockylinux8\n\nGet the path to the container and xstartup:\n\nrealpath rockylinux8/rockylinux8.sif\nrealpath xfce4_config/xstartup",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#installing-hyakvnc",
    "href": "docs/start/hyakvnc.html#installing-hyakvnc",
    "title": "hyakvnc",
    "section": "",
    "text": "From the login node, run the following to install or upgrade hyakvnc:\npython3.6 -m pip install --upgrade pip\npython3.6 -m pip install git+https://github.com/uw-psych/hyakvnc\nOnce installed, run hyakvnc -h to print usage help:\n$ hyakvnc -h\nusage: hyakvnc [-h] [-d] [-v] [-J &lt;job_name&gt;] {create,status,kill,kill-all,set-passwd,repair} ...\n\npositional arguments:\n  {create,status,kill,kill-all,set-passwd,repair}\n    create              Create VNC session\n    status              Print details of all VNC jobs with given job name and exit\n    kill                Kill specified job\n    kill-all            Cancel all VNC jobs with given job name and exit\n    set-passwd          Prompts for new VNC password and exit\n    repair              Repair all missing/broken LoginNode&lt;-&gt;SubNode port forwards, and then exit\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d, --debug           Enable debug logging\n  -v, --version         Print program version and exit\n  -J &lt;job_name&gt;         Slurm job name\nCheck usage help for a specific hyakvnc command by running hyakvnc &lt;command&gt; -h.",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#creating-a-vnc-session",
    "href": "docs/start/hyakvnc.html#creating-a-vnc-session",
    "title": "hyakvnc",
    "section": "",
    "text": "VNC session cannot be reserved indefinitely and will terminate when its time expires.\n\n\n\n\n\n\n\nNote\n\n\n\nIf hyakvnc create fails to reserve a node and timesout, then check if you are reducing too many compute resources.\n\n\n\nFrom the login node shell, we will need to use hyakvnc create command to start the VNC session with compute resources. As an example, to create a VNC session running on our rockylinux8 container on a compute node reserved for 10 hours with specified amount of compute resources, run the following:\n\nhyakvnc create \\\n    -A &lt;lab&gt; \\\n    -p &lt;node_type&gt; \\\n    --cpus 8 \\\n    --gpus 0 \\\n    --mem 32G \\\n    --time 10 \\\n    --container /path/to/hyak_vnc_apptainer/rockylinux8/rockylinux8.sif \\\n    --xstartup /path/to/hyak_vnc_apptainer/xfce4_config/xstartup\nOn initial use, accept to create an SSH key and set a VNC password when prompted. If you want to reset your VNC password, run hyakvnc set-passwd from the login node.\n\nCreate a port forward to Klone to access the VNC session.\n\nIf hyakvnc create was successful, it should print additional steps to setup a port forward and to connect to the VNC session.\n...\n=====================\nRun the following in a new terminal window:\n        ssh -N -f -L 590A:127.0.0.1:590B UWNetID@klone.hyak.uw.edu\nthen connect to VNC session at localhost:590A\n=====================\nIn this example, we want to create a port forward to Klone with the source port 590A and destination at 127.0.0.1:590B or localhost:590B.\n\nOpenSSH (macOS, Linux, Windows)\n\n\nCreate a new terminal window on your local computer and run the ssh command as instructed by hyakvnc create:\nssh -N -f -L 590A:127.0.0.1:590B UWNetID@klone.hyak.uw.edu",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#putty",
    "href": "docs/start/hyakvnc.html#putty",
    "title": "hyakvnc",
    "section": "",
    "text": "Instead of creating a new terminal window, right-click the title bar of an existing terminal window, then select Change Settings:\n\n\n\nNavigate to Connection-&gt;SSH-&gt;Tunnels. Set Source port to 590A and Destination to 127.0.0.1:590B.\n\n\n\nClick on Add to add the port forward, then click on Apply at the bottom of the window.\n\n\n\n\n\n\n\nNote\n\n\n\nClosing all PuTTY terminal windows will close the port forward and break the VNC connection.\n\n\n::: :::\n\nOpen TigerVNC Viewer and connect to the VNC session at localhost:590A as instructed by hyakvnc create:\n\n\n\nLogin with your VNC password:\n\n\nIf successful, you should be greeted to a graphical desktop interface.",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#checking-active-vnc-sessions",
    "href": "docs/start/hyakvnc.html#checking-active-vnc-sessions",
    "title": "hyakvnc",
    "section": "",
    "text": "Running hyakvnc status from the login node will print details of active VNC sessions (with the same job name), including the following: - Slurm Job ID - Node hostname - Time left in days-hours:minutes:seconds - VNC status - VNC port forward command\nActive hyakvnc jobs:\n        Job ID: AAAAAAAA\n                SubNode: n3301\n                Time left: 2-06:00:16\n                VNC active: True\n                VNC display number: 2\n                VNC port: 5902\n                Mapped LoginNode port: 5900\n                Run command: ssh -N -f -L 5900:127.0.0.1:5900 UWNetID@klone.hyak.uw.edu\n        Job ID: BBBBBBBB\n                SubNode: n3301\n                Time left: 2-01:24:25\n                VNC active: True\n                VNC display number: 1\n                VNC port: 5901\n                Mapped LoginNode port: 5911\n                Run command: ssh -N -f -L 5911:127.0.0.1:5911 UWNetID@klone.hyak.uw.edu\n        Job ID: CCCCCCCC\n                SubNode: g3071\n                Time left: 15:07:56\n                VNC active: True\n                VNC display number: 1\n                VNC port: 5901\n                Mapped LoginNode port: 5902\n                Run command: ssh -N -f -L 5902:127.0.0.1:5902 UWNetID@klone.hyak.uw.edu",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#closing-vnc-sessions",
    "href": "docs/start/hyakvnc.html#closing-vnc-sessions",
    "title": "hyakvnc",
    "section": "",
    "text": "To close all VNC sessions, run hyakvnc kill-all from the login node.\nTo close a specific VNC session, find its Job ID with hyakvnc status, then run hyakvnc kill &lt;job_ID&gt; from the login node.",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#using-lmod-modules-in-vnc-session",
    "href": "docs/start/hyakvnc.html#using-lmod-modules-in-vnc-session",
    "title": "hyakvnc",
    "section": "",
    "text": "Note\n\n\n\nRequires the VNC apptainer to support Lmod. /sw/ must also be bounded to access existing module files.\n\n\nIf the VNC apptainer supports Lmod, then module command should work normally without throwing an error.",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#using-apptainer-module-in-vnc-session",
    "href": "docs/start/hyakvnc.html#using-apptainer-module-in-vnc-session",
    "title": "hyakvnc",
    "section": "",
    "text": "Note\n\n\n\nRequires the VNC apptainer to support Lmod and supply Apptainer dependencies.\n\n\nLoad apptainer module with a version specified (using default will not work unless apptainer is installed in the container):\nmodule load apptainer/1.1.5\nThen apptainer commands will work normally:\nExample:\napptainer shell &lt;container.sif&gt;\napptainer exec &lt;container.sif&gt; &lt;command&gt;\n\n\nTo load a nested Apptainer within the VNC session with CUDA support, add the following arguments: --nv -B /.singularity.d.\nExample:\napptainer shell --nv -B /.singularity.d &lt;container.sif&gt;",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  },
  {
    "objectID": "docs/start/hyakvnc.html#references",
    "href": "docs/start/hyakvnc.html#references",
    "title": "hyakvnc",
    "section": "",
    "text": "https://github.com/uw-psych/hyakvnc",
    "crumbs": [
      "Getting Started",
      "hyakvnc"
    ]
  }
]